//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-30411180
// Cuda compilation tools, release 11.5, V11.5.50
// Based on NVVM 7.0.1
//

.version 7.5
.target sm_75
.address_size 64

	// .globl	BlockSum_int64
.weak .global .align 4 .b8 _ZZN4cuda3std3__48__detail21__stronger_order_cudaEiiE7__xform[16] = {3, 0, 0, 0, 4, 0, 0, 0, 4, 0, 0, 0, 3, 0, 0, 0};
.weak .shared .align 8 .u64 _ZZ8BlockSumIlET_PKS0_lE9block_sum;
.weak .shared .align 8 .u64 _ZZ12are_all_nansIdEbRKN18cooperative_groups4__v112thread_blockEPKT_lE5count;
.weak .shared .align 8 .f64 _ZZ8BlockSumIdET_PKS0_lE9block_sum;
.weak .shared .align 8 .u64 _ZZ9BlockMeanIlEdPKT_lE9block_sum;
.weak .shared .align 8 .f64 _ZZ9BlockMeanIdEdPKT_lE9block_sum;
.weak .shared .align 8 .f64 _ZZ8BlockVarIlEdPKT_lE9block_var;
.weak .shared .align 8 .u64 _ZZ8BlockVarIlEdPKT_lE9block_sum;
.weak .shared .align 8 .f64 _ZZ8BlockVarIdEdPKT_lE9block_var;
.weak .shared .align 8 .f64 _ZZ8BlockVarIdEdPKT_lE9block_sum;
.weak .shared .align 8 .u64 _ZZ8BlockMinIlET_PKS0_lE9block_min;
.weak .shared .align 8 .f64 _ZZ8BlockMinIdET_PKS0_lE9block_min;
.weak .shared .align 8 .u64 _ZZ8BlockMaxIlET_PKS0_lE9block_max;
.weak .shared .align 8 .f64 _ZZ8BlockMaxIdET_PKS0_lE9block_max;
.weak .shared .align 8 .u64 _ZZ11BlockIdxMinIlElPKT_PllE9block_min;
.weak .shared .align 8 .u64 _ZZ11BlockIdxMinIlElPKT_PllE13block_idx_min;
.weak .shared .align 1 .u8 _ZZ11BlockIdxMinIlElPKT_PllE9found_min;
.weak .shared .align 8 .f64 _ZZ11BlockIdxMinIdElPKT_PllE9block_min;
.weak .shared .align 8 .u64 _ZZ11BlockIdxMinIdElPKT_PllE13block_idx_min;
.weak .shared .align 1 .u8 _ZZ11BlockIdxMinIdElPKT_PllE9found_min;
.weak .shared .align 8 .u64 _ZZ11BlockIdxMaxIlElPKT_PllE9block_max;
.weak .shared .align 8 .u64 _ZZ11BlockIdxMaxIlElPKT_PllE13block_idx_max;
.weak .shared .align 1 .u8 _ZZ11BlockIdxMaxIlElPKT_PllE9found_max;
.weak .shared .align 8 .f64 _ZZ11BlockIdxMaxIdElPKT_PllE9block_max;
.weak .shared .align 8 .u64 _ZZ11BlockIdxMaxIdElPKT_PllE13block_idx_max;
.weak .shared .align 1 .u8 _ZZ11BlockIdxMaxIdElPKT_PllE9found_max;
.const .align 1 .b8 __nv_static_33__3665470f_11_function_cu_538f90f9__ZN42_INTERNAL_3665470f_11_function_cu_538f90f94cudf7strings6detail19max_string_sentinelE[5] = {247, 191, 191, 191, 0};
.global .align 1 .b8 __nv_static_33__3665470f_11_function_cu_538f90f9__ZN42_INTERNAL_3665470f_11_function_cu_538f90f96thrust6system6detail10sequential3seqE[1];
.global .align 1 .b8 __nv_static_33__3665470f_11_function_cu_538f90f9__ZN42_INTERNAL_3665470f_11_function_cu_538f90f96thrust6system3cpp3parE[1];
.global .align 1 .b8 __nv_static_33__3665470f_11_function_cu_538f90f9__ZN42_INTERNAL_3665470f_11_function_cu_538f90f96thrust8cuda_cub3parE[1];
.global .align 1 .b8 __nv_static_33__3665470f_11_function_cu_538f90f9__ZN42_INTERNAL_3665470f_11_function_cu_538f90f96thrust8cuda_cub10par_nosyncE[1];
.global .align 1 .b8 __nv_static_33__3665470f_11_function_cu_538f90f9__ZN42_INTERNAL_3665470f_11_function_cu_538f90f96thrust12placeholders2_1E[1];
.global .align 1 .b8 __nv_static_33__3665470f_11_function_cu_538f90f9__ZN42_INTERNAL_3665470f_11_function_cu_538f90f96thrust12placeholders2_2E[1];
.global .align 1 .b8 __nv_static_33__3665470f_11_function_cu_538f90f9__ZN42_INTERNAL_3665470f_11_function_cu_538f90f96thrust12placeholders2_3E[1];
.global .align 1 .b8 __nv_static_33__3665470f_11_function_cu_538f90f9__ZN42_INTERNAL_3665470f_11_function_cu_538f90f96thrust12placeholders2_4E[1];
.global .align 1 .b8 __nv_static_33__3665470f_11_function_cu_538f90f9__ZN42_INTERNAL_3665470f_11_function_cu_538f90f96thrust12placeholders2_5E[1];
.global .align 1 .b8 __nv_static_33__3665470f_11_function_cu_538f90f9__ZN42_INTERNAL_3665470f_11_function_cu_538f90f96thrust12placeholders2_6E[1];
.global .align 1 .b8 __nv_static_33__3665470f_11_function_cu_538f90f9__ZN42_INTERNAL_3665470f_11_function_cu_538f90f96thrust12placeholders2_7E[1];
.global .align 1 .b8 __nv_static_33__3665470f_11_function_cu_538f90f9__ZN42_INTERNAL_3665470f_11_function_cu_538f90f96thrust12placeholders2_8E[1];
.global .align 1 .b8 __nv_static_33__3665470f_11_function_cu_538f90f9__ZN42_INTERNAL_3665470f_11_function_cu_538f90f96thrust12placeholders2_9E[1];
.global .align 1 .b8 __nv_static_33__3665470f_11_function_cu_538f90f9__ZN42_INTERNAL_3665470f_11_function_cu_538f90f96thrust12placeholders3_10E[1];
.global .align 1 .b8 __nv_static_33__3665470f_11_function_cu_538f90f9__ZN42_INTERNAL_3665470f_11_function_cu_538f90f96thrust3seqE[1];
.global .align 1 .b8 __nv_static_33__3665470f_11_function_cu_538f90f9__ZN42_INTERNAL_3665470f_11_function_cu_538f90f96thrust6deviceE[1];

.visible .func  (.param .b32 func_retval0) BlockSum_int64(
	.param .b64 BlockSum_int64_param_0,
	.param .b64 BlockSum_int64_param_1,
	.param .b64 BlockSum_int64_param_2
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<16>;
	.reg .b64 	%rd<78>;


	ld.param.u64 	%rd32, [BlockSum_int64_param_0];
	ld.param.u64 	%rd33, [BlockSum_int64_param_1];
	ld.param.u64 	%rd34, [BlockSum_int64_param_2];
	mov.u32 	%r1, %ntid.y;
	mov.u32 	%r4, %tid.z;
	mov.u32 	%r5, %tid.y;
	mad.lo.s32 	%r6, %r1, %r4, %r5;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r3, %r6, %r2, %r7;
	setp.ne.s32 	%p1, %r3, 0;
	@%p1 bra 	$L__BB0_2;

	mov.u64 	%rd35, 0;
	st.shared.u64 	[_ZZ8BlockSumIlET_PKS0_lE9block_sum], %rd35;

$L__BB0_2:
	barrier.sync 	0;
	cvt.u64.u32 	%rd71, %r3;
	setp.ge.s64 	%p2, %rd71, %rd34;
	mov.u64 	%rd76, 0;
	@%p2 bra 	$L__BB0_12;

	mul.lo.s32 	%r8, %r2, %r1;
	mov.u32 	%r9, %ntid.z;
	mul.lo.s32 	%r10, %r8, %r9;
	cvt.u64.u32 	%rd2, %r10;
	not.b64 	%rd37, %rd71;
	add.s64 	%rd3, %rd37, %rd34;
	and.b64  	%rd38, %rd3, -4294967296;
	setp.eq.s64 	%p3, %rd38, 0;
	@%p3 bra 	$L__BB0_5;

	div.u64 	%rd65, %rd3, %rd2;
	bra.uni 	$L__BB0_6;

$L__BB0_5:
	cvt.u32.u64 	%r11, %rd2;
	cvt.u32.u64 	%r12, %rd3;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd65, %r13;

$L__BB0_6:
	add.s64 	%rd7, %rd65, 1;
	and.b64  	%rd75, %rd7, 3;
	setp.lt.u64 	%p4, %rd65, 3;
	mov.u64 	%rd76, 0;
	@%p4 bra 	$L__BB0_9;

	sub.s64 	%rd69, %rd7, %rd75;
	shl.b64 	%rd42, %rd71, 3;
	add.s64 	%rd66, %rd33, %rd42;
	shl.b64 	%rd11, %rd2, 3;

$L__BB0_8:
	ld.u64 	%rd43, [%rd66];
	add.s64 	%rd44, %rd43, %rd76;
	add.s64 	%rd45, %rd66, %rd11;
	ld.u64 	%rd46, [%rd45];
	add.s64 	%rd47, %rd46, %rd44;
	add.s64 	%rd48, %rd71, %rd2;
	add.s64 	%rd49, %rd48, %rd2;
	add.s64 	%rd50, %rd45, %rd11;
	ld.u64 	%rd51, [%rd50];
	add.s64 	%rd52, %rd51, %rd47;
	add.s64 	%rd53, %rd49, %rd2;
	add.s64 	%rd54, %rd50, %rd11;
	add.s64 	%rd66, %rd54, %rd11;
	ld.u64 	%rd55, [%rd54];
	add.s64 	%rd76, %rd55, %rd52;
	add.s64 	%rd71, %rd53, %rd2;
	add.s64 	%rd69, %rd69, -4;
	setp.ne.s64 	%p5, %rd69, 0;
	@%p5 bra 	$L__BB0_8;

$L__BB0_9:
	setp.eq.s64 	%p6, %rd75, 0;
	@%p6 bra 	$L__BB0_12;

	shl.b64 	%rd56, %rd71, 3;
	add.s64 	%rd73, %rd33, %rd56;
	shl.b64 	%rd24, %rd2, 3;

$L__BB0_11:
	.pragma "nounroll";
	ld.u64 	%rd57, [%rd73];
	add.s64 	%rd76, %rd57, %rd76;
	add.s64 	%rd73, %rd73, %rd24;
	add.s64 	%rd75, %rd75, -1;
	setp.ne.s64 	%p7, %rd75, 0;
	@%p7 bra 	$L__BB0_11;

$L__BB0_12:
	mov.u32 	%r14, _ZZ8BlockSumIlET_PKS0_lE9block_sum;
	{ .reg .b64 %tmp;
	  cvt.u64.u32 	%tmp, %r14;
	  cvta.shared.u64 	%rd63, %tmp; }
	mov.u64 	%rd77, %rd63;
	mov.u64 	%rd59, %rd77;
	// begin inline asm
	atom.add.relaxed.cta.u64 %rd58,[%rd59],%rd76;
	// end inline asm
	mov.u32 	%r15, 0;
	barrier.sync 	0;
	ld.shared.u64 	%rd64, [_ZZ8BlockSumIlET_PKS0_lE9block_sum];
	st.u64 	[%rd32], %rd64;
	bar.sync 	0;
	st.param.b32 	[func_retval0+0], %r15;
	ret;

}
	// .globl	BlockSum_float64
.visible .func  (.param .b32 func_retval0) BlockSum_float64(
	.param .b64 BlockSum_float64_param_0,
	.param .b64 BlockSum_float64_param_1,
	.param .b64 BlockSum_float64_param_2
)
{
	.local .align 8 .b8 	__local_depot1[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<13>;
	.reg .b32 	%r<20>;
	.reg .f64 	%fd<33>;
	.reg .b64 	%rd<63>;


	mov.u64 	%SPL, __local_depot1;
	ld.param.u64 	%rd27, [BlockSum_float64_param_0];
	ld.param.u64 	%rd28, [BlockSum_float64_param_1];
	ld.param.u64 	%rd29, [BlockSum_float64_param_2];
	mov.u32 	%r1, %ntid.y;
	mov.u32 	%r4, %tid.z;
	mov.u32 	%r5, %tid.y;
	mad.lo.s32 	%r6, %r1, %r4, %r5;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r3, %r6, %r2, %r7;
	setp.ne.s32 	%p1, %r3, 0;
	@%p1 bra 	$L__BB1_2;

	mov.u64 	%rd30, 0;
	st.shared.u64 	[_ZZ12are_all_nansIdEbRKN18cooperative_groups4__v112thread_blockEPKT_lE5count], %rd30;

$L__BB1_2:
	barrier.sync 	0;
	cvt.u64.u32 	%rd60, %r3;
	setp.ge.s64 	%p2, %rd60, %rd29;
	@%p2 bra 	$L__BB1_7;

	add.u64 	%rd2, %SPL, 0;
	mul.lo.s32 	%r8, %r2, %r1;
	mov.u32 	%r9, %ntid.z;
	mul.lo.s32 	%r10, %r8, %r9;
	cvt.u64.u32 	%rd3, %r10;
	mov.u64 	%rd55, %rd60;

$L__BB1_4:
	shl.b64 	%rd32, %rd55, 3;
	add.s64 	%rd33, %rd28, %rd32;
	ld.f64 	%fd10, [%rd33];
	abs.f64 	%fd11, %fd10;
	setp.gtu.f64 	%p3, %fd11, 0d7FF0000000000000;
	@%p3 bra 	$L__BB1_6;
	bra.uni 	$L__BB1_5;

$L__BB1_6:
	add.s64 	%rd55, %rd55, %rd3;
	setp.lt.s64 	%p4, %rd55, %rd29;
	@%p4 bra 	$L__BB1_4;
	bra.uni 	$L__BB1_7;

$L__BB1_5:
	mov.u32 	%r11, _ZZ12are_all_nansIdEbRKN18cooperative_groups4__v112thread_blockEPKT_lE5count;
	{ .reg .b64 %tmp;
	  cvt.u64.u32 	%tmp, %r11;
	  cvta.shared.u64 	%rd37, %tmp; }
	st.local.u64 	[%rd2], %rd37;
	ld.local.u64 	%rd35, [%rd2];
	mov.u64 	%rd36, 1;
	// begin inline asm
	atom.add.relaxed.cta.u64 %rd34,[%rd35],%rd36;
	// end inline asm

$L__BB1_7:
	barrier.sync 	0;
	ld.shared.u64 	%rd38, [_ZZ12are_all_nansIdEbRKN18cooperative_groups4__v112thread_blockEPKT_lE5count];
	setp.eq.s64 	%p5, %rd38, 0;
	mov.f64 	%fd32, 0d0000000000000000;
	@%p5 bra 	$L__BB1_21;

	@%p1 bra 	$L__BB1_10;

	mov.u64 	%rd39, 0;
	st.shared.u64 	[_ZZ8BlockSumIdET_PKS0_lE9block_sum], %rd39;

$L__BB1_10:
	barrier.sync 	0;
	mov.f64 	%fd31, 0d0000000000000000;
	@%p2 bra 	$L__BB1_20;

	mul.lo.s32 	%r12, %r2, %r1;
	mov.u32 	%r13, %ntid.z;
	mul.lo.s32 	%r14, %r12, %r13;
	cvt.u64.u32 	%rd6, %r14;
	not.b64 	%rd40, %rd60;
	add.s64 	%rd7, %rd40, %rd29;
	and.b64  	%rd41, %rd7, -4294967296;
	setp.eq.s64 	%p8, %rd41, 0;
	@%p8 bra 	$L__BB1_13;

	div.u64 	%rd56, %rd7, %rd6;
	bra.uni 	$L__BB1_14;

$L__BB1_13:
	cvt.u32.u64 	%r15, %rd6;
	cvt.u32.u64 	%r16, %rd7;
	div.u32 	%r17, %r16, %r15;
	cvt.u64.u32 	%rd56, %r17;

$L__BB1_14:
	add.s64 	%rd42, %rd56, 1;
	and.b64  	%rd59, %rd42, 3;
	setp.eq.s64 	%p9, %rd59, 0;
	mov.f64 	%fd31, 0d0000000000000000;
	@%p9 bra 	$L__BB1_17;

	shl.b64 	%rd43, %rd60, 3;
	add.s64 	%rd57, %rd28, %rd43;
	shl.b64 	%rd13, %rd6, 3;

$L__BB1_16:
	.pragma "nounroll";
	ld.f64 	%fd17, [%rd57];
	add.f64 	%fd31, %fd31, %fd17;
	add.s64 	%rd60, %rd60, %rd6;
	add.s64 	%rd57, %rd57, %rd13;
	add.s64 	%rd59, %rd59, -1;
	setp.ne.s64 	%p10, %rd59, 0;
	@%p10 bra 	$L__BB1_16;

$L__BB1_17:
	setp.lt.u64 	%p11, %rd56, 3;
	@%p11 bra 	$L__BB1_20;

	shl.b64 	%rd44, %rd60, 3;
	add.s64 	%rd62, %rd28, %rd44;
	shl.b64 	%rd22, %rd6, 3;

$L__BB1_19:
	ld.f64 	%fd18, [%rd62];
	add.f64 	%fd19, %fd31, %fd18;
	add.s64 	%rd45, %rd62, %rd22;
	ld.f64 	%fd20, [%rd45];
	add.f64 	%fd21, %fd19, %fd20;
	add.s64 	%rd46, %rd60, %rd6;
	add.s64 	%rd47, %rd46, %rd6;
	add.s64 	%rd48, %rd45, %rd22;
	ld.f64 	%fd22, [%rd48];
	add.f64 	%fd23, %fd21, %fd22;
	add.s64 	%rd49, %rd47, %rd6;
	add.s64 	%rd50, %rd48, %rd22;
	add.s64 	%rd62, %rd50, %rd22;
	ld.f64 	%fd24, [%rd50];
	add.f64 	%fd31, %fd23, %fd24;
	add.s64 	%rd60, %rd49, %rd6;
	setp.lt.s64 	%p12, %rd60, %rd29;
	@%p12 bra 	$L__BB1_19;

$L__BB1_20:
	add.u64 	%rd53, %SPL, 0;
	mov.u32 	%r18, _ZZ8BlockSumIdET_PKS0_lE9block_sum;
	{ .reg .b64 %tmp;
	  cvt.u64.u32 	%tmp, %r18;
	  cvta.shared.u64 	%rd54, %tmp; }
	st.local.u64 	[%rd53], %rd54;
	ld.local.u64 	%rd51, [%rd53];
	// begin inline asm
	atom.add.relaxed.cta.f64 %fd25,[%rd51],%fd31;
	// end inline asm
	barrier.sync 	0;
	ld.shared.f64 	%fd32, [_ZZ8BlockSumIdET_PKS0_lE9block_sum];

$L__BB1_21:
	st.f64 	[%rd27], %fd32;
	bar.sync 	0;
	mov.u32 	%r19, 0;
	st.param.b32 	[func_retval0+0], %r19;
	ret;

}
	// .globl	BlockMean_int64
.visible .func  (.param .b32 func_retval0) BlockMean_int64(
	.param .b64 BlockMean_int64_param_0,
	.param .b64 BlockMean_int64_param_1,
	.param .b64 BlockMean_int64_param_2
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<16>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<78>;


	ld.param.u64 	%rd32, [BlockMean_int64_param_0];
	ld.param.u64 	%rd33, [BlockMean_int64_param_1];
	ld.param.u64 	%rd34, [BlockMean_int64_param_2];
	mov.u32 	%r1, %ntid.y;
	mov.u32 	%r4, %tid.z;
	mov.u32 	%r5, %tid.y;
	mad.lo.s32 	%r6, %r1, %r4, %r5;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r3, %r6, %r2, %r7;
	setp.ne.s32 	%p1, %r3, 0;
	@%p1 bra 	$L__BB2_2;

	mov.u64 	%rd35, 0;
	st.shared.u64 	[_ZZ9BlockMeanIlEdPKT_lE9block_sum], %rd35;

$L__BB2_2:
	barrier.sync 	0;
	cvt.u64.u32 	%rd71, %r3;
	setp.ge.s64 	%p2, %rd71, %rd34;
	mov.u64 	%rd76, 0;
	@%p2 bra 	$L__BB2_12;

	mul.lo.s32 	%r8, %r2, %r1;
	mov.u32 	%r9, %ntid.z;
	mul.lo.s32 	%r10, %r8, %r9;
	cvt.u64.u32 	%rd2, %r10;
	not.b64 	%rd37, %rd71;
	add.s64 	%rd3, %rd37, %rd34;
	and.b64  	%rd38, %rd3, -4294967296;
	setp.eq.s64 	%p3, %rd38, 0;
	@%p3 bra 	$L__BB2_5;

	div.u64 	%rd65, %rd3, %rd2;
	bra.uni 	$L__BB2_6;

$L__BB2_5:
	cvt.u32.u64 	%r11, %rd2;
	cvt.u32.u64 	%r12, %rd3;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd65, %r13;

$L__BB2_6:
	add.s64 	%rd7, %rd65, 1;
	and.b64  	%rd75, %rd7, 3;
	setp.lt.u64 	%p4, %rd65, 3;
	mov.u64 	%rd76, 0;
	@%p4 bra 	$L__BB2_9;

	sub.s64 	%rd69, %rd7, %rd75;
	shl.b64 	%rd42, %rd71, 3;
	add.s64 	%rd66, %rd33, %rd42;
	shl.b64 	%rd11, %rd2, 3;

$L__BB2_8:
	ld.u64 	%rd43, [%rd66];
	add.s64 	%rd44, %rd43, %rd76;
	add.s64 	%rd45, %rd66, %rd11;
	ld.u64 	%rd46, [%rd45];
	add.s64 	%rd47, %rd46, %rd44;
	add.s64 	%rd48, %rd71, %rd2;
	add.s64 	%rd49, %rd48, %rd2;
	add.s64 	%rd50, %rd45, %rd11;
	ld.u64 	%rd51, [%rd50];
	add.s64 	%rd52, %rd51, %rd47;
	add.s64 	%rd53, %rd49, %rd2;
	add.s64 	%rd54, %rd50, %rd11;
	add.s64 	%rd66, %rd54, %rd11;
	ld.u64 	%rd55, [%rd54];
	add.s64 	%rd76, %rd55, %rd52;
	add.s64 	%rd71, %rd53, %rd2;
	add.s64 	%rd69, %rd69, -4;
	setp.ne.s64 	%p5, %rd69, 0;
	@%p5 bra 	$L__BB2_8;

$L__BB2_9:
	setp.eq.s64 	%p6, %rd75, 0;
	@%p6 bra 	$L__BB2_12;

	shl.b64 	%rd56, %rd71, 3;
	add.s64 	%rd73, %rd33, %rd56;
	shl.b64 	%rd24, %rd2, 3;

$L__BB2_11:
	.pragma "nounroll";
	ld.u64 	%rd57, [%rd73];
	add.s64 	%rd76, %rd57, %rd76;
	add.s64 	%rd73, %rd73, %rd24;
	add.s64 	%rd75, %rd75, -1;
	setp.ne.s64 	%p7, %rd75, 0;
	@%p7 bra 	$L__BB2_11;

$L__BB2_12:
	mov.u32 	%r14, _ZZ9BlockMeanIlEdPKT_lE9block_sum;
	{ .reg .b64 %tmp;
	  cvt.u64.u32 	%tmp, %r14;
	  cvta.shared.u64 	%rd63, %tmp; }
	mov.u64 	%rd77, %rd63;
	mov.u64 	%rd59, %rd77;
	// begin inline asm
	atom.add.relaxed.cta.u64 %rd58,[%rd59],%rd76;
	// end inline asm
	mov.u32 	%r15, 0;
	barrier.sync 	0;
	ld.shared.u64 	%rd64, [_ZZ9BlockMeanIlEdPKT_lE9block_sum];
	cvt.rn.f64.s64 	%fd1, %rd64;
	cvt.rn.f64.s64 	%fd2, %rd34;
	div.rn.f64 	%fd3, %fd1, %fd2;
	st.f64 	[%rd32], %fd3;
	bar.sync 	0;
	st.param.b32 	[func_retval0+0], %r15;
	ret;

}
	// .globl	BlockMean_float64
.visible .func  (.param .b32 func_retval0) BlockMean_float64(
	.param .b64 BlockMean_float64_param_0,
	.param .b64 BlockMean_float64_param_1,
	.param .b64 BlockMean_float64_param_2
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<16>;
	.reg .f64 	%fd<30>;
	.reg .b64 	%rd<50>;


	ld.param.u64 	%rd23, [BlockMean_float64_param_0];
	ld.param.u64 	%rd24, [BlockMean_float64_param_1];
	ld.param.u64 	%rd25, [BlockMean_float64_param_2];
	mov.u32 	%r1, %ntid.y;
	mov.u32 	%r4, %tid.z;
	mov.u32 	%r5, %tid.y;
	mad.lo.s32 	%r6, %r1, %r4, %r5;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r3, %r6, %r2, %r7;
	setp.ne.s32 	%p1, %r3, 0;
	@%p1 bra 	$L__BB3_2;

	mov.u64 	%rd26, 0;
	st.shared.u64 	[_ZZ9BlockMeanIdEdPKT_lE9block_sum], %rd26;

$L__BB3_2:
	barrier.sync 	0;
	cvt.u64.u32 	%rd46, %r3;
	setp.ge.s64 	%p2, %rd46, %rd25;
	mov.f64 	%fd29, 0d0000000000000000;
	@%p2 bra 	$L__BB3_12;

	mul.lo.s32 	%r8, %r2, %r1;
	mov.u32 	%r9, %ntid.z;
	mul.lo.s32 	%r10, %r8, %r9;
	cvt.u64.u32 	%rd2, %r10;
	not.b64 	%rd27, %rd46;
	add.s64 	%rd3, %rd27, %rd25;
	and.b64  	%rd28, %rd3, -4294967296;
	setp.eq.s64 	%p3, %rd28, 0;
	@%p3 bra 	$L__BB3_5;

	div.u64 	%rd42, %rd3, %rd2;
	bra.uni 	$L__BB3_6;

$L__BB3_5:
	cvt.u32.u64 	%r11, %rd2;
	cvt.u32.u64 	%r12, %rd3;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd42, %r13;

$L__BB3_6:
	add.s64 	%rd29, %rd42, 1;
	and.b64  	%rd45, %rd29, 3;
	setp.eq.s64 	%p4, %rd45, 0;
	mov.f64 	%fd29, 0d0000000000000000;
	@%p4 bra 	$L__BB3_9;

	shl.b64 	%rd30, %rd46, 3;
	add.s64 	%rd43, %rd24, %rd30;
	shl.b64 	%rd9, %rd2, 3;

$L__BB3_8:
	.pragma "nounroll";
	ld.f64 	%fd12, [%rd43];
	add.f64 	%fd29, %fd29, %fd12;
	add.s64 	%rd46, %rd46, %rd2;
	add.s64 	%rd43, %rd43, %rd9;
	add.s64 	%rd45, %rd45, -1;
	setp.ne.s64 	%p5, %rd45, 0;
	@%p5 bra 	$L__BB3_8;

$L__BB3_9:
	setp.lt.u64 	%p6, %rd42, 3;
	@%p6 bra 	$L__BB3_12;

	shl.b64 	%rd31, %rd46, 3;
	add.s64 	%rd48, %rd24, %rd31;
	shl.b64 	%rd18, %rd2, 3;

$L__BB3_11:
	ld.f64 	%fd13, [%rd48];
	add.f64 	%fd14, %fd29, %fd13;
	add.s64 	%rd32, %rd48, %rd18;
	ld.f64 	%fd15, [%rd32];
	add.f64 	%fd16, %fd14, %fd15;
	add.s64 	%rd33, %rd46, %rd2;
	add.s64 	%rd34, %rd33, %rd2;
	add.s64 	%rd35, %rd32, %rd18;
	ld.f64 	%fd17, [%rd35];
	add.f64 	%fd18, %fd16, %fd17;
	add.s64 	%rd36, %rd34, %rd2;
	add.s64 	%rd37, %rd35, %rd18;
	add.s64 	%rd48, %rd37, %rd18;
	ld.f64 	%fd19, [%rd37];
	add.f64 	%fd29, %fd18, %fd19;
	add.s64 	%rd46, %rd36, %rd2;
	setp.lt.s64 	%p7, %rd46, %rd25;
	@%p7 bra 	$L__BB3_11;

$L__BB3_12:
	mov.u32 	%r14, _ZZ9BlockMeanIdEdPKT_lE9block_sum;
	{ .reg .b64 %tmp;
	  cvt.u64.u32 	%tmp, %r14;
	  cvta.shared.u64 	%rd41, %tmp; }
	mov.u64 	%rd49, %rd41;
	mov.u64 	%rd38, %rd49;
	// begin inline asm
	atom.add.relaxed.cta.f64 %fd20,[%rd38],%fd29;
	// end inline asm
	mov.u32 	%r15, 0;
	barrier.sync 	0;
	cvt.rn.f64.s64 	%fd22, %rd25;
	ld.shared.f64 	%fd23, [_ZZ9BlockMeanIdEdPKT_lE9block_sum];
	div.rn.f64 	%fd24, %fd23, %fd22;
	st.f64 	[%rd23], %fd24;
	bar.sync 	0;
	st.param.b32 	[func_retval0+0], %r15;
	ret;

}
	// .globl	BlockStd_int64
.visible .func  (.param .b32 func_retval0) BlockStd_int64(
	.param .b64 BlockStd_int64_param_0,
	.param .b64 BlockStd_int64_param_1,
	.param .b64 BlockStd_int64_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<23>;
	.reg .f64 	%fd<40>;
	.reg .b64 	%rd<127>;


	ld.param.u64 	%rd53, [BlockStd_int64_param_0];
	ld.param.u64 	%rd54, [BlockStd_int64_param_1];
	ld.param.u64 	%rd55, [BlockStd_int64_param_2];
	mov.u32 	%r1, %ntid.y;
	mov.u32 	%r4, %tid.z;
	mov.u32 	%r5, %tid.y;
	mad.lo.s32 	%r6, %r1, %r4, %r5;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r3, %r6, %r2, %r7;
	setp.ne.s32 	%p1, %r3, 0;
	@%p1 bra 	$L__BB4_2;

	mov.u64 	%rd56, 0;
	st.shared.u64 	[_ZZ8BlockVarIlEdPKT_lE9block_var], %rd56;
	st.shared.u64 	[_ZZ8BlockVarIlEdPKT_lE9block_sum], %rd56;

$L__BB4_2:
	barrier.sync 	0;
	cvt.u64.u32 	%rd123, %r3;
	setp.ge.s64 	%p2, %rd123, %rd55;
	mov.u64 	%rd118, 0;
	@%p2 bra 	$L__BB4_12;

	mul.lo.s32 	%r8, %r2, %r1;
	mov.u32 	%r9, %ntid.z;
	mul.lo.s32 	%r10, %r8, %r9;
	cvt.u64.u32 	%rd2, %r10;
	not.b64 	%rd58, %rd123;
	add.s64 	%rd3, %rd58, %rd55;
	and.b64  	%rd59, %rd3, -4294967296;
	setp.eq.s64 	%p3, %rd59, 0;
	@%p3 bra 	$L__BB4_5;

	div.u64 	%rd107, %rd3, %rd2;
	bra.uni 	$L__BB4_6;

$L__BB4_5:
	cvt.u32.u64 	%r11, %rd2;
	cvt.u32.u64 	%r12, %rd3;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd107, %r13;

$L__BB4_6:
	add.s64 	%rd7, %rd107, 1;
	and.b64  	%rd117, %rd7, 3;
	setp.lt.u64 	%p4, %rd107, 3;
	mov.u64 	%rd118, 0;
	mov.u64 	%rd113, %rd123;
	@%p4 bra 	$L__BB4_9;

	sub.s64 	%rd111, %rd7, %rd117;
	shl.b64 	%rd63, %rd123, 3;
	add.s64 	%rd108, %rd54, %rd63;
	shl.b64 	%rd11, %rd2, 3;
	mov.u64 	%rd113, %rd123;

$L__BB4_8:
	ld.u64 	%rd64, [%rd108];
	add.s64 	%rd65, %rd64, %rd118;
	add.s64 	%rd66, %rd108, %rd11;
	ld.u64 	%rd67, [%rd66];
	add.s64 	%rd68, %rd67, %rd65;
	add.s64 	%rd69, %rd113, %rd2;
	add.s64 	%rd70, %rd69, %rd2;
	add.s64 	%rd71, %rd66, %rd11;
	ld.u64 	%rd72, [%rd71];
	add.s64 	%rd73, %rd72, %rd68;
	add.s64 	%rd74, %rd70, %rd2;
	add.s64 	%rd75, %rd71, %rd11;
	add.s64 	%rd108, %rd75, %rd11;
	ld.u64 	%rd76, [%rd75];
	add.s64 	%rd118, %rd76, %rd73;
	add.s64 	%rd113, %rd74, %rd2;
	add.s64 	%rd111, %rd111, -4;
	setp.ne.s64 	%p5, %rd111, 0;
	@%p5 bra 	$L__BB4_8;

$L__BB4_9:
	setp.eq.s64 	%p6, %rd117, 0;
	@%p6 bra 	$L__BB4_12;

	shl.b64 	%rd77, %rd113, 3;
	add.s64 	%rd115, %rd54, %rd77;
	shl.b64 	%rd24, %rd2, 3;

$L__BB4_11:
	.pragma "nounroll";
	ld.u64 	%rd78, [%rd115];
	add.s64 	%rd118, %rd78, %rd118;
	add.s64 	%rd115, %rd115, %rd24;
	add.s64 	%rd117, %rd117, -1;
	setp.ne.s64 	%p7, %rd117, 0;
	@%p7 bra 	$L__BB4_11;

$L__BB4_12:
	mov.u32 	%r14, _ZZ8BlockVarIlEdPKT_lE9block_sum;
	{ .reg .b64 %tmp;
	  cvt.u64.u32 	%tmp, %r14;
	  cvta.shared.u64 	%rd84, %tmp; }
	mov.u64 	%rd126, %rd84;
	mov.u64 	%rd80, %rd126;
	// begin inline asm
	atom.add.relaxed.cta.u64 %rd79,[%rd80],%rd118;
	// end inline asm
	barrier.sync 	0;
	ld.shared.u64 	%rd85, [_ZZ8BlockVarIlEdPKT_lE9block_sum];
	cvt.rn.f64.s64 	%fd10, %rd85;
	cvt.rn.f64.s64 	%fd11, %rd55;
	div.rn.f64 	%fd1, %fd10, %fd11;
	mov.f64 	%fd39, 0d0000000000000000;
	@%p2 bra 	$L__BB4_22;

	mul.lo.s32 	%r15, %r2, %r1;
	mov.u32 	%r16, %ntid.z;
	mul.lo.s32 	%r17, %r15, %r16;
	cvt.u64.u32 	%rd32, %r17;
	not.b64 	%rd86, %rd123;
	add.s64 	%rd33, %rd86, %rd55;
	and.b64  	%rd87, %rd33, -4294967296;
	setp.eq.s64 	%p9, %rd87, 0;
	@%p9 bra 	$L__BB4_15;

	div.u64 	%rd119, %rd33, %rd32;
	bra.uni 	$L__BB4_16;

$L__BB4_15:
	cvt.u32.u64 	%r18, %rd32;
	cvt.u32.u64 	%r19, %rd33;
	div.u32 	%r20, %r19, %r18;
	cvt.u64.u32 	%rd119, %r20;

$L__BB4_16:
	add.s64 	%rd88, %rd119, 1;
	and.b64  	%rd122, %rd88, 3;
	setp.eq.s64 	%p10, %rd122, 0;
	mov.f64 	%fd39, 0d0000000000000000;
	@%p10 bra 	$L__BB4_19;

	shl.b64 	%rd89, %rd123, 3;
	add.s64 	%rd120, %rd54, %rd89;
	shl.b64 	%rd39, %rd32, 3;

$L__BB4_18:
	.pragma "nounroll";
	ld.u64 	%rd90, [%rd120];
	cvt.rn.f64.s64 	%fd15, %rd90;
	sub.f64 	%fd16, %fd15, %fd1;
	fma.rn.f64 	%fd39, %fd16, %fd16, %fd39;
	add.s64 	%rd123, %rd123, %rd32;
	add.s64 	%rd120, %rd120, %rd39;
	add.s64 	%rd122, %rd122, -1;
	setp.ne.s64 	%p11, %rd122, 0;
	@%p11 bra 	$L__BB4_18;

$L__BB4_19:
	setp.lt.u64 	%p12, %rd119, 3;
	@%p12 bra 	$L__BB4_22;

	shl.b64 	%rd91, %rd123, 3;
	add.s64 	%rd125, %rd54, %rd91;
	shl.b64 	%rd48, %rd32, 3;

$L__BB4_21:
	ld.u64 	%rd92, [%rd125];
	cvt.rn.f64.s64 	%fd17, %rd92;
	sub.f64 	%fd18, %fd17, %fd1;
	fma.rn.f64 	%fd19, %fd18, %fd18, %fd39;
	add.s64 	%rd93, %rd125, %rd48;
	ld.u64 	%rd94, [%rd93];
	cvt.rn.f64.s64 	%fd20, %rd94;
	sub.f64 	%fd21, %fd20, %fd1;
	fma.rn.f64 	%fd22, %fd21, %fd21, %fd19;
	add.s64 	%rd95, %rd123, %rd32;
	add.s64 	%rd96, %rd95, %rd32;
	add.s64 	%rd97, %rd93, %rd48;
	ld.u64 	%rd98, [%rd97];
	cvt.rn.f64.s64 	%fd23, %rd98;
	sub.f64 	%fd24, %fd23, %fd1;
	fma.rn.f64 	%fd25, %fd24, %fd24, %fd22;
	add.s64 	%rd99, %rd96, %rd32;
	add.s64 	%rd100, %rd97, %rd48;
	add.s64 	%rd125, %rd100, %rd48;
	ld.u64 	%rd101, [%rd100];
	cvt.rn.f64.s64 	%fd26, %rd101;
	sub.f64 	%fd27, %fd26, %fd1;
	fma.rn.f64 	%fd39, %fd27, %fd27, %fd25;
	add.s64 	%rd123, %rd99, %rd32;
	setp.lt.s64 	%p13, %rd123, %rd55;
	@%p13 bra 	$L__BB4_21;

$L__BB4_22:
	mov.u32 	%r21, _ZZ8BlockVarIlEdPKT_lE9block_var;
	{ .reg .b64 %tmp;
	  cvt.u64.u32 	%tmp, %r21;
	  cvta.shared.u64 	%rd105, %tmp; }
	mov.u64 	%rd126, %rd105;
	mov.u64 	%rd102, %rd126;
	// begin inline asm
	atom.add.relaxed.cta.f64 %fd28,[%rd102],%fd39;
	// end inline asm
	barrier.sync 	0;
	@%p1 bra 	$L__BB4_24;

	ld.shared.f64 	%fd30, [_ZZ8BlockVarIlEdPKT_lE9block_var];
	add.s64 	%rd106, %rd55, -1;
	cvt.rn.f64.s64 	%fd31, %rd106;
	div.rn.f64 	%fd32, %fd30, %fd31;
	st.shared.f64 	[_ZZ8BlockVarIlEdPKT_lE9block_var], %fd32;

$L__BB4_24:
	mov.u32 	%r22, 0;
	barrier.sync 	0;
	ld.shared.f64 	%fd33, [_ZZ8BlockVarIlEdPKT_lE9block_var];
	sqrt.rn.f64 	%fd34, %fd33;
	st.f64 	[%rd53], %fd34;
	bar.sync 	0;
	st.param.b32 	[func_retval0+0], %r22;
	ret;

}
	// .globl	BlockStd_float64
.visible .func  (.param .b32 func_retval0) BlockStd_float64(
	.param .b64 BlockStd_float64_param_0,
	.param .b64 BlockStd_float64_param_1,
	.param .b64 BlockStd_float64_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<23>;
	.reg .f64 	%fd<66>;
	.reg .b64 	%rd<94>;


	ld.param.u64 	%rd44, [BlockStd_float64_param_0];
	ld.param.u64 	%rd45, [BlockStd_float64_param_1];
	ld.param.u64 	%rd46, [BlockStd_float64_param_2];
	mov.u32 	%r1, %ntid.y;
	mov.u32 	%r4, %tid.z;
	mov.u32 	%r5, %tid.y;
	mad.lo.s32 	%r6, %r1, %r4, %r5;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r3, %r6, %r2, %r7;
	setp.ne.s32 	%p1, %r3, 0;
	@%p1 bra 	$L__BB5_2;

	mov.u64 	%rd47, 0;
	st.shared.u64 	[_ZZ8BlockVarIdEdPKT_lE9block_var], %rd47;
	st.shared.u64 	[_ZZ8BlockVarIdEdPKT_lE9block_sum], %rd47;

$L__BB5_2:
	barrier.sync 	0;
	cvt.u64.u32 	%rd90, %r3;
	setp.ge.s64 	%p2, %rd90, %rd46;
	mov.f64 	%fd65, 0d0000000000000000;
	mov.f64 	%fd60, %fd65;
	@%p2 bra 	$L__BB5_12;

	mul.lo.s32 	%r8, %r2, %r1;
	mov.u32 	%r9, %ntid.z;
	mul.lo.s32 	%r10, %r8, %r9;
	cvt.u64.u32 	%rd2, %r10;
	not.b64 	%rd48, %rd90;
	add.s64 	%rd3, %rd48, %rd46;
	and.b64  	%rd49, %rd3, -4294967296;
	setp.eq.s64 	%p3, %rd49, 0;
	@%p3 bra 	$L__BB5_5;

	div.u64 	%rd79, %rd3, %rd2;
	bra.uni 	$L__BB5_6;

$L__BB5_5:
	cvt.u32.u64 	%r11, %rd2;
	cvt.u32.u64 	%r12, %rd3;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd79, %r13;

$L__BB5_6:
	add.s64 	%rd50, %rd79, 1;
	and.b64  	%rd82, %rd50, 3;
	setp.eq.s64 	%p4, %rd82, 0;
	mov.f64 	%fd60, 0d0000000000000000;
	mov.u64 	%rd83, %rd90;
	@%p4 bra 	$L__BB5_9;

	shl.b64 	%rd51, %rd90, 3;
	add.s64 	%rd80, %rd45, %rd51;
	shl.b64 	%rd9, %rd2, 3;
	mov.u64 	%rd83, %rd90;

$L__BB5_8:
	.pragma "nounroll";
	ld.f64 	%fd20, [%rd80];
	add.f64 	%fd60, %fd60, %fd20;
	add.s64 	%rd83, %rd83, %rd2;
	add.s64 	%rd80, %rd80, %rd9;
	add.s64 	%rd82, %rd82, -1;
	setp.ne.s64 	%p5, %rd82, 0;
	@%p5 bra 	$L__BB5_8;

$L__BB5_9:
	setp.lt.u64 	%p6, %rd79, 3;
	@%p6 bra 	$L__BB5_12;

	shl.b64 	%rd52, %rd83, 3;
	add.s64 	%rd85, %rd45, %rd52;
	shl.b64 	%rd18, %rd2, 3;

$L__BB5_11:
	ld.f64 	%fd21, [%rd85];
	add.f64 	%fd22, %fd60, %fd21;
	add.s64 	%rd53, %rd85, %rd18;
	ld.f64 	%fd23, [%rd53];
	add.f64 	%fd24, %fd22, %fd23;
	add.s64 	%rd54, %rd83, %rd2;
	add.s64 	%rd55, %rd54, %rd2;
	add.s64 	%rd56, %rd53, %rd18;
	ld.f64 	%fd25, [%rd56];
	add.f64 	%fd26, %fd24, %fd25;
	add.s64 	%rd57, %rd55, %rd2;
	add.s64 	%rd58, %rd56, %rd18;
	add.s64 	%rd85, %rd58, %rd18;
	ld.f64 	%fd27, [%rd58];
	add.f64 	%fd60, %fd26, %fd27;
	add.s64 	%rd83, %rd57, %rd2;
	setp.lt.s64 	%p7, %rd83, %rd46;
	@%p7 bra 	$L__BB5_11;

$L__BB5_12:
	mov.u32 	%r14, _ZZ8BlockVarIdEdPKT_lE9block_sum;
	{ .reg .b64 %tmp;
	  cvt.u64.u32 	%tmp, %r14;
	  cvta.shared.u64 	%rd62, %tmp; }
	mov.u64 	%rd93, %rd62;
	mov.u64 	%rd59, %rd93;
	// begin inline asm
	atom.add.relaxed.cta.f64 %fd28,[%rd59],%fd60;
	// end inline asm
	barrier.sync 	0;
	cvt.rn.f64.s64 	%fd31, %rd46;
	ld.shared.f64 	%fd32, [_ZZ8BlockVarIdEdPKT_lE9block_sum];
	div.rn.f64 	%fd8, %fd32, %fd31;
	@%p2 bra 	$L__BB5_22;

	mul.lo.s32 	%r15, %r2, %r1;
	mov.u32 	%r16, %ntid.z;
	mul.lo.s32 	%r17, %r15, %r16;
	cvt.u64.u32 	%rd23, %r17;
	not.b64 	%rd63, %rd90;
	add.s64 	%rd24, %rd63, %rd46;
	and.b64  	%rd64, %rd24, -4294967296;
	setp.eq.s64 	%p9, %rd64, 0;
	@%p9 bra 	$L__BB5_15;

	div.u64 	%rd86, %rd24, %rd23;
	bra.uni 	$L__BB5_16;

$L__BB5_15:
	cvt.u32.u64 	%r18, %rd23;
	cvt.u32.u64 	%r19, %rd24;
	div.u32 	%r20, %r19, %r18;
	cvt.u64.u32 	%rd86, %r20;

$L__BB5_16:
	add.s64 	%rd65, %rd86, 1;
	and.b64  	%rd89, %rd65, 3;
	setp.eq.s64 	%p10, %rd89, 0;
	mov.f64 	%fd65, 0d0000000000000000;
	@%p10 bra 	$L__BB5_19;

	shl.b64 	%rd66, %rd90, 3;
	add.s64 	%rd87, %rd45, %rd66;
	shl.b64 	%rd30, %rd23, 3;

$L__BB5_18:
	.pragma "nounroll";
	ld.f64 	%fd36, [%rd87];
	sub.f64 	%fd37, %fd36, %fd8;
	fma.rn.f64 	%fd65, %fd37, %fd37, %fd65;
	add.s64 	%rd90, %rd90, %rd23;
	add.s64 	%rd87, %rd87, %rd30;
	add.s64 	%rd89, %rd89, -1;
	setp.ne.s64 	%p11, %rd89, 0;
	@%p11 bra 	$L__BB5_18;

$L__BB5_19:
	setp.lt.u64 	%p12, %rd86, 3;
	@%p12 bra 	$L__BB5_22;

	shl.b64 	%rd67, %rd90, 3;
	add.s64 	%rd92, %rd45, %rd67;
	shl.b64 	%rd39, %rd23, 3;

$L__BB5_21:
	ld.f64 	%fd38, [%rd92];
	sub.f64 	%fd39, %fd38, %fd8;
	fma.rn.f64 	%fd40, %fd39, %fd39, %fd65;
	add.s64 	%rd68, %rd92, %rd39;
	ld.f64 	%fd41, [%rd68];
	sub.f64 	%fd42, %fd41, %fd8;
	fma.rn.f64 	%fd43, %fd42, %fd42, %fd40;
	add.s64 	%rd69, %rd90, %rd23;
	add.s64 	%rd70, %rd69, %rd23;
	add.s64 	%rd71, %rd68, %rd39;
	ld.f64 	%fd44, [%rd71];
	sub.f64 	%fd45, %fd44, %fd8;
	fma.rn.f64 	%fd46, %fd45, %fd45, %fd43;
	add.s64 	%rd72, %rd70, %rd23;
	add.s64 	%rd73, %rd71, %rd39;
	add.s64 	%rd92, %rd73, %rd39;
	ld.f64 	%fd47, [%rd73];
	sub.f64 	%fd48, %fd47, %fd8;
	fma.rn.f64 	%fd65, %fd48, %fd48, %fd46;
	add.s64 	%rd90, %rd72, %rd23;
	setp.lt.s64 	%p13, %rd90, %rd46;
	@%p13 bra 	$L__BB5_21;

$L__BB5_22:
	mov.u32 	%r21, _ZZ8BlockVarIdEdPKT_lE9block_var;
	{ .reg .b64 %tmp;
	  cvt.u64.u32 	%tmp, %r21;
	  cvta.shared.u64 	%rd77, %tmp; }
	mov.u64 	%rd93, %rd77;
	mov.u64 	%rd74, %rd93;
	// begin inline asm
	atom.add.relaxed.cta.f64 %fd49,[%rd74],%fd65;
	// end inline asm
	barrier.sync 	0;
	@%p1 bra 	$L__BB5_24;

	ld.shared.f64 	%fd51, [_ZZ8BlockVarIdEdPKT_lE9block_var];
	add.s64 	%rd78, %rd46, -1;
	cvt.rn.f64.s64 	%fd52, %rd78;
	div.rn.f64 	%fd53, %fd51, %fd52;
	st.shared.f64 	[_ZZ8BlockVarIdEdPKT_lE9block_var], %fd53;

$L__BB5_24:
	mov.u32 	%r22, 0;
	barrier.sync 	0;
	ld.shared.f64 	%fd54, [_ZZ8BlockVarIdEdPKT_lE9block_var];
	sqrt.rn.f64 	%fd55, %fd54;
	st.f64 	[%rd44], %fd55;
	bar.sync 	0;
	st.param.b32 	[func_retval0+0], %r22;
	ret;

}
	// .globl	BlockVar_int64
.visible .func  (.param .b32 func_retval0) BlockVar_int64(
	.param .b64 BlockVar_int64_param_0,
	.param .b64 BlockVar_int64_param_1,
	.param .b64 BlockVar_int64_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<23>;
	.reg .f64 	%fd<39>;
	.reg .b64 	%rd<127>;


	ld.param.u64 	%rd53, [BlockVar_int64_param_0];
	ld.param.u64 	%rd54, [BlockVar_int64_param_1];
	ld.param.u64 	%rd55, [BlockVar_int64_param_2];
	mov.u32 	%r1, %ntid.y;
	mov.u32 	%r4, %tid.z;
	mov.u32 	%r5, %tid.y;
	mad.lo.s32 	%r6, %r1, %r4, %r5;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r3, %r6, %r2, %r7;
	setp.ne.s32 	%p1, %r3, 0;
	@%p1 bra 	$L__BB6_2;

	mov.u64 	%rd56, 0;
	st.shared.u64 	[_ZZ8BlockVarIlEdPKT_lE9block_var], %rd56;
	st.shared.u64 	[_ZZ8BlockVarIlEdPKT_lE9block_sum], %rd56;

$L__BB6_2:
	barrier.sync 	0;
	cvt.u64.u32 	%rd123, %r3;
	setp.ge.s64 	%p2, %rd123, %rd55;
	mov.u64 	%rd118, 0;
	@%p2 bra 	$L__BB6_12;

	mul.lo.s32 	%r8, %r2, %r1;
	mov.u32 	%r9, %ntid.z;
	mul.lo.s32 	%r10, %r8, %r9;
	cvt.u64.u32 	%rd2, %r10;
	not.b64 	%rd58, %rd123;
	add.s64 	%rd3, %rd58, %rd55;
	and.b64  	%rd59, %rd3, -4294967296;
	setp.eq.s64 	%p3, %rd59, 0;
	@%p3 bra 	$L__BB6_5;

	div.u64 	%rd107, %rd3, %rd2;
	bra.uni 	$L__BB6_6;

$L__BB6_5:
	cvt.u32.u64 	%r11, %rd2;
	cvt.u32.u64 	%r12, %rd3;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd107, %r13;

$L__BB6_6:
	add.s64 	%rd7, %rd107, 1;
	and.b64  	%rd117, %rd7, 3;
	setp.lt.u64 	%p4, %rd107, 3;
	mov.u64 	%rd118, 0;
	mov.u64 	%rd113, %rd123;
	@%p4 bra 	$L__BB6_9;

	sub.s64 	%rd111, %rd7, %rd117;
	shl.b64 	%rd63, %rd123, 3;
	add.s64 	%rd108, %rd54, %rd63;
	shl.b64 	%rd11, %rd2, 3;
	mov.u64 	%rd113, %rd123;

$L__BB6_8:
	ld.u64 	%rd64, [%rd108];
	add.s64 	%rd65, %rd64, %rd118;
	add.s64 	%rd66, %rd108, %rd11;
	ld.u64 	%rd67, [%rd66];
	add.s64 	%rd68, %rd67, %rd65;
	add.s64 	%rd69, %rd113, %rd2;
	add.s64 	%rd70, %rd69, %rd2;
	add.s64 	%rd71, %rd66, %rd11;
	ld.u64 	%rd72, [%rd71];
	add.s64 	%rd73, %rd72, %rd68;
	add.s64 	%rd74, %rd70, %rd2;
	add.s64 	%rd75, %rd71, %rd11;
	add.s64 	%rd108, %rd75, %rd11;
	ld.u64 	%rd76, [%rd75];
	add.s64 	%rd118, %rd76, %rd73;
	add.s64 	%rd113, %rd74, %rd2;
	add.s64 	%rd111, %rd111, -4;
	setp.ne.s64 	%p5, %rd111, 0;
	@%p5 bra 	$L__BB6_8;

$L__BB6_9:
	setp.eq.s64 	%p6, %rd117, 0;
	@%p6 bra 	$L__BB6_12;

	shl.b64 	%rd77, %rd113, 3;
	add.s64 	%rd115, %rd54, %rd77;
	shl.b64 	%rd24, %rd2, 3;

$L__BB6_11:
	.pragma "nounroll";
	ld.u64 	%rd78, [%rd115];
	add.s64 	%rd118, %rd78, %rd118;
	add.s64 	%rd115, %rd115, %rd24;
	add.s64 	%rd117, %rd117, -1;
	setp.ne.s64 	%p7, %rd117, 0;
	@%p7 bra 	$L__BB6_11;

$L__BB6_12:
	mov.u32 	%r14, _ZZ8BlockVarIlEdPKT_lE9block_sum;
	{ .reg .b64 %tmp;
	  cvt.u64.u32 	%tmp, %r14;
	  cvta.shared.u64 	%rd84, %tmp; }
	mov.u64 	%rd126, %rd84;
	mov.u64 	%rd80, %rd126;
	// begin inline asm
	atom.add.relaxed.cta.u64 %rd79,[%rd80],%rd118;
	// end inline asm
	barrier.sync 	0;
	ld.shared.u64 	%rd85, [_ZZ8BlockVarIlEdPKT_lE9block_sum];
	cvt.rn.f64.s64 	%fd10, %rd85;
	cvt.rn.f64.s64 	%fd11, %rd55;
	div.rn.f64 	%fd1, %fd10, %fd11;
	mov.f64 	%fd38, 0d0000000000000000;
	@%p2 bra 	$L__BB6_22;

	mul.lo.s32 	%r15, %r2, %r1;
	mov.u32 	%r16, %ntid.z;
	mul.lo.s32 	%r17, %r15, %r16;
	cvt.u64.u32 	%rd32, %r17;
	not.b64 	%rd86, %rd123;
	add.s64 	%rd33, %rd86, %rd55;
	and.b64  	%rd87, %rd33, -4294967296;
	setp.eq.s64 	%p9, %rd87, 0;
	@%p9 bra 	$L__BB6_15;

	div.u64 	%rd119, %rd33, %rd32;
	bra.uni 	$L__BB6_16;

$L__BB6_15:
	cvt.u32.u64 	%r18, %rd32;
	cvt.u32.u64 	%r19, %rd33;
	div.u32 	%r20, %r19, %r18;
	cvt.u64.u32 	%rd119, %r20;

$L__BB6_16:
	add.s64 	%rd88, %rd119, 1;
	and.b64  	%rd122, %rd88, 3;
	setp.eq.s64 	%p10, %rd122, 0;
	mov.f64 	%fd38, 0d0000000000000000;
	@%p10 bra 	$L__BB6_19;

	shl.b64 	%rd89, %rd123, 3;
	add.s64 	%rd120, %rd54, %rd89;
	shl.b64 	%rd39, %rd32, 3;

$L__BB6_18:
	.pragma "nounroll";
	ld.u64 	%rd90, [%rd120];
	cvt.rn.f64.s64 	%fd15, %rd90;
	sub.f64 	%fd16, %fd15, %fd1;
	fma.rn.f64 	%fd38, %fd16, %fd16, %fd38;
	add.s64 	%rd123, %rd123, %rd32;
	add.s64 	%rd120, %rd120, %rd39;
	add.s64 	%rd122, %rd122, -1;
	setp.ne.s64 	%p11, %rd122, 0;
	@%p11 bra 	$L__BB6_18;

$L__BB6_19:
	setp.lt.u64 	%p12, %rd119, 3;
	@%p12 bra 	$L__BB6_22;

	shl.b64 	%rd91, %rd123, 3;
	add.s64 	%rd125, %rd54, %rd91;
	shl.b64 	%rd48, %rd32, 3;

$L__BB6_21:
	ld.u64 	%rd92, [%rd125];
	cvt.rn.f64.s64 	%fd17, %rd92;
	sub.f64 	%fd18, %fd17, %fd1;
	fma.rn.f64 	%fd19, %fd18, %fd18, %fd38;
	add.s64 	%rd93, %rd125, %rd48;
	ld.u64 	%rd94, [%rd93];
	cvt.rn.f64.s64 	%fd20, %rd94;
	sub.f64 	%fd21, %fd20, %fd1;
	fma.rn.f64 	%fd22, %fd21, %fd21, %fd19;
	add.s64 	%rd95, %rd123, %rd32;
	add.s64 	%rd96, %rd95, %rd32;
	add.s64 	%rd97, %rd93, %rd48;
	ld.u64 	%rd98, [%rd97];
	cvt.rn.f64.s64 	%fd23, %rd98;
	sub.f64 	%fd24, %fd23, %fd1;
	fma.rn.f64 	%fd25, %fd24, %fd24, %fd22;
	add.s64 	%rd99, %rd96, %rd32;
	add.s64 	%rd100, %rd97, %rd48;
	add.s64 	%rd125, %rd100, %rd48;
	ld.u64 	%rd101, [%rd100];
	cvt.rn.f64.s64 	%fd26, %rd101;
	sub.f64 	%fd27, %fd26, %fd1;
	fma.rn.f64 	%fd38, %fd27, %fd27, %fd25;
	add.s64 	%rd123, %rd99, %rd32;
	setp.lt.s64 	%p13, %rd123, %rd55;
	@%p13 bra 	$L__BB6_21;

$L__BB6_22:
	mov.u32 	%r21, _ZZ8BlockVarIlEdPKT_lE9block_var;
	{ .reg .b64 %tmp;
	  cvt.u64.u32 	%tmp, %r21;
	  cvta.shared.u64 	%rd105, %tmp; }
	mov.u64 	%rd126, %rd105;
	mov.u64 	%rd102, %rd126;
	// begin inline asm
	atom.add.relaxed.cta.f64 %fd28,[%rd102],%fd38;
	// end inline asm
	barrier.sync 	0;
	@%p1 bra 	$L__BB6_24;

	ld.shared.f64 	%fd30, [_ZZ8BlockVarIlEdPKT_lE9block_var];
	add.s64 	%rd106, %rd55, -1;
	cvt.rn.f64.s64 	%fd31, %rd106;
	div.rn.f64 	%fd32, %fd30, %fd31;
	st.shared.f64 	[_ZZ8BlockVarIlEdPKT_lE9block_var], %fd32;

$L__BB6_24:
	mov.u32 	%r22, 0;
	barrier.sync 	0;
	ld.shared.f64 	%fd33, [_ZZ8BlockVarIlEdPKT_lE9block_var];
	st.f64 	[%rd53], %fd33;
	bar.sync 	0;
	st.param.b32 	[func_retval0+0], %r22;
	ret;

}
	// .globl	BlockVar_float64
.visible .func  (.param .b32 func_retval0) BlockVar_float64(
	.param .b64 BlockVar_float64_param_0,
	.param .b64 BlockVar_float64_param_1,
	.param .b64 BlockVar_float64_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<23>;
	.reg .f64 	%fd<65>;
	.reg .b64 	%rd<94>;


	ld.param.u64 	%rd44, [BlockVar_float64_param_0];
	ld.param.u64 	%rd45, [BlockVar_float64_param_1];
	ld.param.u64 	%rd46, [BlockVar_float64_param_2];
	mov.u32 	%r1, %ntid.y;
	mov.u32 	%r4, %tid.z;
	mov.u32 	%r5, %tid.y;
	mad.lo.s32 	%r6, %r1, %r4, %r5;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r3, %r6, %r2, %r7;
	setp.ne.s32 	%p1, %r3, 0;
	@%p1 bra 	$L__BB7_2;

	mov.u64 	%rd47, 0;
	st.shared.u64 	[_ZZ8BlockVarIdEdPKT_lE9block_var], %rd47;
	st.shared.u64 	[_ZZ8BlockVarIdEdPKT_lE9block_sum], %rd47;

$L__BB7_2:
	barrier.sync 	0;
	cvt.u64.u32 	%rd90, %r3;
	setp.ge.s64 	%p2, %rd90, %rd46;
	mov.f64 	%fd64, 0d0000000000000000;
	mov.f64 	%fd59, %fd64;
	@%p2 bra 	$L__BB7_12;

	mul.lo.s32 	%r8, %r2, %r1;
	mov.u32 	%r9, %ntid.z;
	mul.lo.s32 	%r10, %r8, %r9;
	cvt.u64.u32 	%rd2, %r10;
	not.b64 	%rd48, %rd90;
	add.s64 	%rd3, %rd48, %rd46;
	and.b64  	%rd49, %rd3, -4294967296;
	setp.eq.s64 	%p3, %rd49, 0;
	@%p3 bra 	$L__BB7_5;

	div.u64 	%rd79, %rd3, %rd2;
	bra.uni 	$L__BB7_6;

$L__BB7_5:
	cvt.u32.u64 	%r11, %rd2;
	cvt.u32.u64 	%r12, %rd3;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd79, %r13;

$L__BB7_6:
	add.s64 	%rd50, %rd79, 1;
	and.b64  	%rd82, %rd50, 3;
	setp.eq.s64 	%p4, %rd82, 0;
	mov.f64 	%fd59, 0d0000000000000000;
	mov.u64 	%rd83, %rd90;
	@%p4 bra 	$L__BB7_9;

	shl.b64 	%rd51, %rd90, 3;
	add.s64 	%rd80, %rd45, %rd51;
	shl.b64 	%rd9, %rd2, 3;
	mov.u64 	%rd83, %rd90;

$L__BB7_8:
	.pragma "nounroll";
	ld.f64 	%fd20, [%rd80];
	add.f64 	%fd59, %fd59, %fd20;
	add.s64 	%rd83, %rd83, %rd2;
	add.s64 	%rd80, %rd80, %rd9;
	add.s64 	%rd82, %rd82, -1;
	setp.ne.s64 	%p5, %rd82, 0;
	@%p5 bra 	$L__BB7_8;

$L__BB7_9:
	setp.lt.u64 	%p6, %rd79, 3;
	@%p6 bra 	$L__BB7_12;

	shl.b64 	%rd52, %rd83, 3;
	add.s64 	%rd85, %rd45, %rd52;
	shl.b64 	%rd18, %rd2, 3;

$L__BB7_11:
	ld.f64 	%fd21, [%rd85];
	add.f64 	%fd22, %fd59, %fd21;
	add.s64 	%rd53, %rd85, %rd18;
	ld.f64 	%fd23, [%rd53];
	add.f64 	%fd24, %fd22, %fd23;
	add.s64 	%rd54, %rd83, %rd2;
	add.s64 	%rd55, %rd54, %rd2;
	add.s64 	%rd56, %rd53, %rd18;
	ld.f64 	%fd25, [%rd56];
	add.f64 	%fd26, %fd24, %fd25;
	add.s64 	%rd57, %rd55, %rd2;
	add.s64 	%rd58, %rd56, %rd18;
	add.s64 	%rd85, %rd58, %rd18;
	ld.f64 	%fd27, [%rd58];
	add.f64 	%fd59, %fd26, %fd27;
	add.s64 	%rd83, %rd57, %rd2;
	setp.lt.s64 	%p7, %rd83, %rd46;
	@%p7 bra 	$L__BB7_11;

$L__BB7_12:
	mov.u32 	%r14, _ZZ8BlockVarIdEdPKT_lE9block_sum;
	{ .reg .b64 %tmp;
	  cvt.u64.u32 	%tmp, %r14;
	  cvta.shared.u64 	%rd62, %tmp; }
	mov.u64 	%rd93, %rd62;
	mov.u64 	%rd59, %rd93;
	// begin inline asm
	atom.add.relaxed.cta.f64 %fd28,[%rd59],%fd59;
	// end inline asm
	barrier.sync 	0;
	cvt.rn.f64.s64 	%fd31, %rd46;
	ld.shared.f64 	%fd32, [_ZZ8BlockVarIdEdPKT_lE9block_sum];
	div.rn.f64 	%fd8, %fd32, %fd31;
	@%p2 bra 	$L__BB7_22;

	mul.lo.s32 	%r15, %r2, %r1;
	mov.u32 	%r16, %ntid.z;
	mul.lo.s32 	%r17, %r15, %r16;
	cvt.u64.u32 	%rd23, %r17;
	not.b64 	%rd63, %rd90;
	add.s64 	%rd24, %rd63, %rd46;
	and.b64  	%rd64, %rd24, -4294967296;
	setp.eq.s64 	%p9, %rd64, 0;
	@%p9 bra 	$L__BB7_15;

	div.u64 	%rd86, %rd24, %rd23;
	bra.uni 	$L__BB7_16;

$L__BB7_15:
	cvt.u32.u64 	%r18, %rd23;
	cvt.u32.u64 	%r19, %rd24;
	div.u32 	%r20, %r19, %r18;
	cvt.u64.u32 	%rd86, %r20;

$L__BB7_16:
	add.s64 	%rd65, %rd86, 1;
	and.b64  	%rd89, %rd65, 3;
	setp.eq.s64 	%p10, %rd89, 0;
	mov.f64 	%fd64, 0d0000000000000000;
	@%p10 bra 	$L__BB7_19;

	shl.b64 	%rd66, %rd90, 3;
	add.s64 	%rd87, %rd45, %rd66;
	shl.b64 	%rd30, %rd23, 3;

$L__BB7_18:
	.pragma "nounroll";
	ld.f64 	%fd36, [%rd87];
	sub.f64 	%fd37, %fd36, %fd8;
	fma.rn.f64 	%fd64, %fd37, %fd37, %fd64;
	add.s64 	%rd90, %rd90, %rd23;
	add.s64 	%rd87, %rd87, %rd30;
	add.s64 	%rd89, %rd89, -1;
	setp.ne.s64 	%p11, %rd89, 0;
	@%p11 bra 	$L__BB7_18;

$L__BB7_19:
	setp.lt.u64 	%p12, %rd86, 3;
	@%p12 bra 	$L__BB7_22;

	shl.b64 	%rd67, %rd90, 3;
	add.s64 	%rd92, %rd45, %rd67;
	shl.b64 	%rd39, %rd23, 3;

$L__BB7_21:
	ld.f64 	%fd38, [%rd92];
	sub.f64 	%fd39, %fd38, %fd8;
	fma.rn.f64 	%fd40, %fd39, %fd39, %fd64;
	add.s64 	%rd68, %rd92, %rd39;
	ld.f64 	%fd41, [%rd68];
	sub.f64 	%fd42, %fd41, %fd8;
	fma.rn.f64 	%fd43, %fd42, %fd42, %fd40;
	add.s64 	%rd69, %rd90, %rd23;
	add.s64 	%rd70, %rd69, %rd23;
	add.s64 	%rd71, %rd68, %rd39;
	ld.f64 	%fd44, [%rd71];
	sub.f64 	%fd45, %fd44, %fd8;
	fma.rn.f64 	%fd46, %fd45, %fd45, %fd43;
	add.s64 	%rd72, %rd70, %rd23;
	add.s64 	%rd73, %rd71, %rd39;
	add.s64 	%rd92, %rd73, %rd39;
	ld.f64 	%fd47, [%rd73];
	sub.f64 	%fd48, %fd47, %fd8;
	fma.rn.f64 	%fd64, %fd48, %fd48, %fd46;
	add.s64 	%rd90, %rd72, %rd23;
	setp.lt.s64 	%p13, %rd90, %rd46;
	@%p13 bra 	$L__BB7_21;

$L__BB7_22:
	mov.u32 	%r21, _ZZ8BlockVarIdEdPKT_lE9block_var;
	{ .reg .b64 %tmp;
	  cvt.u64.u32 	%tmp, %r21;
	  cvta.shared.u64 	%rd77, %tmp; }
	mov.u64 	%rd93, %rd77;
	mov.u64 	%rd74, %rd93;
	// begin inline asm
	atom.add.relaxed.cta.f64 %fd49,[%rd74],%fd64;
	// end inline asm
	barrier.sync 	0;
	@%p1 bra 	$L__BB7_24;

	ld.shared.f64 	%fd51, [_ZZ8BlockVarIdEdPKT_lE9block_var];
	add.s64 	%rd78, %rd46, -1;
	cvt.rn.f64.s64 	%fd52, %rd78;
	div.rn.f64 	%fd53, %fd51, %fd52;
	st.shared.f64 	[_ZZ8BlockVarIdEdPKT_lE9block_var], %fd53;

$L__BB7_24:
	mov.u32 	%r22, 0;
	barrier.sync 	0;
	ld.shared.f64 	%fd54, [_ZZ8BlockVarIdEdPKT_lE9block_var];
	st.f64 	[%rd44], %fd54;
	bar.sync 	0;
	st.param.b32 	[func_retval0+0], %r22;
	ret;

}
	// .globl	BlockMin_int64
.visible .func  (.param .b32 func_retval0) BlockMin_int64(
	.param .b64 BlockMin_int64_param_0,
	.param .b64 BlockMin_int64_param_1,
	.param .b64 BlockMin_int64_param_2
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<16>;
	.reg .b64 	%rd<78>;


	ld.param.u64 	%rd32, [BlockMin_int64_param_0];
	ld.param.u64 	%rd33, [BlockMin_int64_param_1];
	ld.param.u64 	%rd34, [BlockMin_int64_param_2];
	mov.u32 	%r1, %ntid.y;
	mov.u32 	%r4, %tid.z;
	mov.u32 	%r5, %tid.y;
	mad.lo.s32 	%r6, %r1, %r4, %r5;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r3, %r6, %r2, %r7;
	setp.ne.s32 	%p1, %r3, 0;
	@%p1 bra 	$L__BB8_2;

	mov.u64 	%rd35, 9223372036854775807;
	st.shared.u64 	[_ZZ8BlockMinIlET_PKS0_lE9block_min], %rd35;

$L__BB8_2:
	barrier.sync 	0;
	cvt.u64.u32 	%rd71, %r3;
	setp.ge.s64 	%p2, %rd71, %rd34;
	mov.u64 	%rd76, 9223372036854775807;
	@%p2 bra 	$L__BB8_12;

	mul.lo.s32 	%r8, %r2, %r1;
	mov.u32 	%r9, %ntid.z;
	mul.lo.s32 	%r10, %r8, %r9;
	cvt.u64.u32 	%rd2, %r10;
	not.b64 	%rd37, %rd71;
	add.s64 	%rd3, %rd37, %rd34;
	and.b64  	%rd38, %rd3, -4294967296;
	setp.eq.s64 	%p3, %rd38, 0;
	@%p3 bra 	$L__BB8_5;

	div.u64 	%rd65, %rd3, %rd2;
	bra.uni 	$L__BB8_6;

$L__BB8_5:
	cvt.u32.u64 	%r11, %rd2;
	cvt.u32.u64 	%r12, %rd3;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd65, %r13;

$L__BB8_6:
	add.s64 	%rd7, %rd65, 1;
	and.b64  	%rd75, %rd7, 3;
	setp.lt.u64 	%p4, %rd65, 3;
	mov.u64 	%rd76, 9223372036854775807;
	@%p4 bra 	$L__BB8_9;

	sub.s64 	%rd69, %rd7, %rd75;
	shl.b64 	%rd42, %rd71, 3;
	add.s64 	%rd66, %rd33, %rd42;
	shl.b64 	%rd11, %rd2, 3;

$L__BB8_8:
	ld.u64 	%rd43, [%rd66];
	min.s64 	%rd44, %rd76, %rd43;
	add.s64 	%rd45, %rd66, %rd11;
	ld.u64 	%rd46, [%rd45];
	min.s64 	%rd47, %rd44, %rd46;
	add.s64 	%rd48, %rd71, %rd2;
	add.s64 	%rd49, %rd48, %rd2;
	add.s64 	%rd50, %rd45, %rd11;
	ld.u64 	%rd51, [%rd50];
	min.s64 	%rd52, %rd47, %rd51;
	add.s64 	%rd53, %rd49, %rd2;
	add.s64 	%rd54, %rd50, %rd11;
	add.s64 	%rd66, %rd54, %rd11;
	ld.u64 	%rd55, [%rd54];
	min.s64 	%rd76, %rd52, %rd55;
	add.s64 	%rd71, %rd53, %rd2;
	add.s64 	%rd69, %rd69, -4;
	setp.ne.s64 	%p5, %rd69, 0;
	@%p5 bra 	$L__BB8_8;

$L__BB8_9:
	setp.eq.s64 	%p6, %rd75, 0;
	@%p6 bra 	$L__BB8_12;

	shl.b64 	%rd56, %rd71, 3;
	add.s64 	%rd73, %rd33, %rd56;
	shl.b64 	%rd24, %rd2, 3;

$L__BB8_11:
	.pragma "nounroll";
	ld.u64 	%rd57, [%rd73];
	min.s64 	%rd76, %rd76, %rd57;
	add.s64 	%rd73, %rd73, %rd24;
	add.s64 	%rd75, %rd75, -1;
	setp.ne.s64 	%p7, %rd75, 0;
	@%p7 bra 	$L__BB8_11;

$L__BB8_12:
	mov.u32 	%r14, _ZZ8BlockMinIlET_PKS0_lE9block_min;
	{ .reg .b64 %tmp;
	  cvt.u64.u32 	%tmp, %r14;
	  cvta.shared.u64 	%rd63, %tmp; }
	mov.u64 	%rd77, %rd63;
	mov.u64 	%rd59, %rd77;
	// begin inline asm
	atom.min.relaxed.cta.s64 %rd58,[%rd59],%rd76;
	// end inline asm
	mov.u32 	%r15, 0;
	barrier.sync 	0;
	ld.shared.u64 	%rd64, [_ZZ8BlockMinIlET_PKS0_lE9block_min];
	st.u64 	[%rd32], %rd64;
	bar.sync 	0;
	st.param.b32 	[func_retval0+0], %r15;
	ret;

}
	// .globl	BlockMin_float64
.visible .func  (.param .b32 func_retval0) BlockMin_float64(
	.param .b64 BlockMin_float64_param_0,
	.param .b64 BlockMin_float64_param_1,
	.param .b64 BlockMin_float64_param_2
)
{
	.local .align 8 .b8 	__local_depot9[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<19>;
	.reg .b32 	%r<20>;
	.reg .f64 	%fd<38>;
	.reg .b64 	%rd<69>;


	mov.u64 	%SPL, __local_depot9;
	ld.param.u64 	%rd28, [BlockMin_float64_param_0];
	ld.param.u64 	%rd29, [BlockMin_float64_param_1];
	ld.param.u64 	%rd30, [BlockMin_float64_param_2];
	mov.u32 	%r1, %ntid.y;
	mov.u32 	%r4, %tid.z;
	mov.u32 	%r5, %tid.y;
	mad.lo.s32 	%r6, %r1, %r4, %r5;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r3, %r6, %r2, %r7;
	setp.ne.s32 	%p3, %r3, 0;
	@%p3 bra 	$L__BB9_2;

	mov.u64 	%rd31, 0;
	st.shared.u64 	[_ZZ12are_all_nansIdEbRKN18cooperative_groups4__v112thread_blockEPKT_lE5count], %rd31;

$L__BB9_2:
	barrier.sync 	0;
	cvt.u64.u32 	%rd66, %r3;
	setp.ge.s64 	%p4, %rd66, %rd30;
	@%p4 bra 	$L__BB9_7;

	add.u64 	%rd2, %SPL, 0;
	mul.lo.s32 	%r8, %r2, %r1;
	mov.u32 	%r9, %ntid.z;
	mul.lo.s32 	%r10, %r8, %r9;
	cvt.u64.u32 	%rd3, %r10;
	mov.u64 	%rd61, %rd66;

$L__BB9_4:
	shl.b64 	%rd33, %rd61, 3;
	add.s64 	%rd34, %rd29, %rd33;
	ld.f64 	%fd15, [%rd34];
	abs.f64 	%fd16, %fd15;
	setp.gtu.f64 	%p5, %fd16, 0d7FF0000000000000;
	@%p5 bra 	$L__BB9_6;
	bra.uni 	$L__BB9_5;

$L__BB9_6:
	add.s64 	%rd61, %rd61, %rd3;
	setp.lt.s64 	%p6, %rd61, %rd30;
	@%p6 bra 	$L__BB9_4;
	bra.uni 	$L__BB9_7;

$L__BB9_5:
	mov.u32 	%r11, _ZZ12are_all_nansIdEbRKN18cooperative_groups4__v112thread_blockEPKT_lE5count;
	{ .reg .b64 %tmp;
	  cvt.u64.u32 	%tmp, %r11;
	  cvta.shared.u64 	%rd38, %tmp; }
	st.local.u64 	[%rd2], %rd38;
	ld.local.u64 	%rd36, [%rd2];
	mov.u64 	%rd37, 1;
	// begin inline asm
	atom.add.relaxed.cta.u64 %rd35,[%rd36],%rd37;
	// end inline asm

$L__BB9_7:
	barrier.sync 	0;
	ld.shared.u64 	%rd39, [_ZZ12are_all_nansIdEbRKN18cooperative_groups4__v112thread_blockEPKT_lE5count];
	setp.eq.s64 	%p7, %rd39, 0;
	mov.f64 	%fd37, 0d7FF8000000000000;
	@%p7 bra 	$L__BB9_25;

	@%p3 bra 	$L__BB9_10;

	mov.u64 	%rd40, 9218868437227405312;
	st.shared.u64 	[_ZZ8BlockMinIdET_PKS0_lE9block_min], %rd40;

$L__BB9_10:
	barrier.sync 	0;
	mov.f64 	%fd34, 0d7FF0000000000000;
	@%p4 bra 	$L__BB9_20;

	mul.lo.s32 	%r12, %r2, %r1;
	mov.u32 	%r13, %ntid.z;
	mul.lo.s32 	%r14, %r12, %r13;
	cvt.u64.u32 	%rd6, %r14;
	not.b64 	%rd41, %rd66;
	add.s64 	%rd7, %rd41, %rd30;
	and.b64  	%rd42, %rd7, -4294967296;
	setp.eq.s64 	%p10, %rd42, 0;
	@%p10 bra 	$L__BB9_13;

	div.u64 	%rd62, %rd7, %rd6;
	bra.uni 	$L__BB9_14;

$L__BB9_13:
	cvt.u32.u64 	%r15, %rd6;
	cvt.u32.u64 	%r16, %rd7;
	div.u32 	%r17, %r16, %r15;
	cvt.u64.u32 	%rd62, %r17;

$L__BB9_14:
	add.s64 	%rd43, %rd62, 1;
	and.b64  	%rd65, %rd43, 3;
	setp.eq.s64 	%p11, %rd65, 0;
	mov.f64 	%fd34, 0d7FF0000000000000;
	@%p11 bra 	$L__BB9_17;

	shl.b64 	%rd44, %rd66, 3;
	add.s64 	%rd63, %rd29, %rd44;
	shl.b64 	%rd13, %rd6, 3;

$L__BB9_16:
	.pragma "nounroll";
	ld.f64 	%fd22, [%rd63];
	min.f64 	%fd34, %fd34, %fd22;
	add.s64 	%rd66, %rd66, %rd6;
	add.s64 	%rd63, %rd63, %rd13;
	add.s64 	%rd65, %rd65, -1;
	setp.ne.s64 	%p12, %rd65, 0;
	@%p12 bra 	$L__BB9_16;

$L__BB9_17:
	setp.lt.u64 	%p13, %rd62, 3;
	@%p13 bra 	$L__BB9_20;

	shl.b64 	%rd45, %rd66, 3;
	add.s64 	%rd68, %rd29, %rd45;
	shl.b64 	%rd22, %rd6, 3;

$L__BB9_19:
	ld.f64 	%fd23, [%rd68];
	min.f64 	%fd24, %fd34, %fd23;
	add.s64 	%rd46, %rd68, %rd22;
	ld.f64 	%fd25, [%rd46];
	min.f64 	%fd26, %fd24, %fd25;
	add.s64 	%rd47, %rd66, %rd6;
	add.s64 	%rd48, %rd47, %rd6;
	add.s64 	%rd49, %rd46, %rd22;
	ld.f64 	%fd27, [%rd49];
	min.f64 	%fd28, %fd26, %fd27;
	add.s64 	%rd50, %rd48, %rd6;
	add.s64 	%rd51, %rd49, %rd22;
	add.s64 	%rd68, %rd51, %rd22;
	ld.f64 	%fd29, [%rd51];
	min.f64 	%fd34, %fd28, %fd29;
	add.s64 	%rd66, %rd50, %rd6;
	setp.lt.s64 	%p14, %rd66, %rd30;
	@%p14 bra 	$L__BB9_19;

$L__BB9_20:
	add.u64 	%rd55, %SPL, 0;
	mov.u32 	%r18, _ZZ8BlockMinIdET_PKS0_lE9block_min;
	{ .reg .b64 %tmp;
	  cvt.u64.u32 	%tmp, %r18;
	  cvta.shared.u64 	%rd56, %tmp; }
	st.local.u64 	[%rd55], %rd56;
	ld.local.u64 	%rd27, [%rd55];
	// begin inline asm
	ld.relaxed.cta.b64 %rd52,[%rd27];
	// end inline asm
	mov.b64 	%fd36, %rd52;
	mov.pred 	%p15, 0;

$L__BB9_21:
	setp.lt.f64 	%p16, %fd36, %fd34;
	selp.f64 	%fd10, %fd36, %fd34, %p16;
	setp.neu.f64 	%p17, %fd10, %fd34;
	mov.pred 	%p18, %p15;
	@%p17 bra 	$L__BB9_23;

	mov.b64 	%rd59, %fd36;
	mov.b64 	%rd60, %fd10;
	// begin inline asm
	atom.cas.relaxed.cta.b64 %rd57,[%rd27],%rd59,%rd60;
	// end inline asm
	setp.ne.s64 	%p18, %rd57, %rd59;
	mov.b64 	%fd36, %rd57;

$L__BB9_23:
	@%p18 bra 	$L__BB9_21;

	barrier.sync 	0;
	ld.shared.f64 	%fd37, [_ZZ8BlockMinIdET_PKS0_lE9block_min];

$L__BB9_25:
	st.f64 	[%rd28], %fd37;
	bar.sync 	0;
	mov.u32 	%r19, 0;
	st.param.b32 	[func_retval0+0], %r19;
	ret;

}
	// .globl	BlockMax_int64
.visible .func  (.param .b32 func_retval0) BlockMax_int64(
	.param .b64 BlockMax_int64_param_0,
	.param .b64 BlockMax_int64_param_1,
	.param .b64 BlockMax_int64_param_2
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<16>;
	.reg .b64 	%rd<78>;


	ld.param.u64 	%rd32, [BlockMax_int64_param_0];
	ld.param.u64 	%rd33, [BlockMax_int64_param_1];
	ld.param.u64 	%rd34, [BlockMax_int64_param_2];
	mov.u32 	%r1, %ntid.y;
	mov.u32 	%r4, %tid.z;
	mov.u32 	%r5, %tid.y;
	mad.lo.s32 	%r6, %r1, %r4, %r5;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r3, %r6, %r2, %r7;
	setp.ne.s32 	%p1, %r3, 0;
	@%p1 bra 	$L__BB10_2;

	mov.u64 	%rd35, -9223372036854775808;
	st.shared.u64 	[_ZZ8BlockMaxIlET_PKS0_lE9block_max], %rd35;

$L__BB10_2:
	barrier.sync 	0;
	cvt.u64.u32 	%rd71, %r3;
	setp.ge.s64 	%p2, %rd71, %rd34;
	mov.u64 	%rd76, -9223372036854775808;
	@%p2 bra 	$L__BB10_12;

	mul.lo.s32 	%r8, %r2, %r1;
	mov.u32 	%r9, %ntid.z;
	mul.lo.s32 	%r10, %r8, %r9;
	cvt.u64.u32 	%rd2, %r10;
	not.b64 	%rd37, %rd71;
	add.s64 	%rd3, %rd37, %rd34;
	and.b64  	%rd38, %rd3, -4294967296;
	setp.eq.s64 	%p3, %rd38, 0;
	@%p3 bra 	$L__BB10_5;

	div.u64 	%rd65, %rd3, %rd2;
	bra.uni 	$L__BB10_6;

$L__BB10_5:
	cvt.u32.u64 	%r11, %rd2;
	cvt.u32.u64 	%r12, %rd3;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd65, %r13;

$L__BB10_6:
	add.s64 	%rd7, %rd65, 1;
	and.b64  	%rd75, %rd7, 3;
	setp.lt.u64 	%p4, %rd65, 3;
	mov.u64 	%rd76, -9223372036854775808;
	@%p4 bra 	$L__BB10_9;

	sub.s64 	%rd69, %rd7, %rd75;
	shl.b64 	%rd42, %rd71, 3;
	add.s64 	%rd66, %rd33, %rd42;
	shl.b64 	%rd11, %rd2, 3;

$L__BB10_8:
	ld.u64 	%rd43, [%rd66];
	max.s64 	%rd44, %rd76, %rd43;
	add.s64 	%rd45, %rd66, %rd11;
	ld.u64 	%rd46, [%rd45];
	max.s64 	%rd47, %rd44, %rd46;
	add.s64 	%rd48, %rd71, %rd2;
	add.s64 	%rd49, %rd48, %rd2;
	add.s64 	%rd50, %rd45, %rd11;
	ld.u64 	%rd51, [%rd50];
	max.s64 	%rd52, %rd47, %rd51;
	add.s64 	%rd53, %rd49, %rd2;
	add.s64 	%rd54, %rd50, %rd11;
	add.s64 	%rd66, %rd54, %rd11;
	ld.u64 	%rd55, [%rd54];
	max.s64 	%rd76, %rd52, %rd55;
	add.s64 	%rd71, %rd53, %rd2;
	add.s64 	%rd69, %rd69, -4;
	setp.ne.s64 	%p5, %rd69, 0;
	@%p5 bra 	$L__BB10_8;

$L__BB10_9:
	setp.eq.s64 	%p6, %rd75, 0;
	@%p6 bra 	$L__BB10_12;

	shl.b64 	%rd56, %rd71, 3;
	add.s64 	%rd73, %rd33, %rd56;
	shl.b64 	%rd24, %rd2, 3;

$L__BB10_11:
	.pragma "nounroll";
	ld.u64 	%rd57, [%rd73];
	max.s64 	%rd76, %rd76, %rd57;
	add.s64 	%rd73, %rd73, %rd24;
	add.s64 	%rd75, %rd75, -1;
	setp.ne.s64 	%p7, %rd75, 0;
	@%p7 bra 	$L__BB10_11;

$L__BB10_12:
	mov.u32 	%r14, _ZZ8BlockMaxIlET_PKS0_lE9block_max;
	{ .reg .b64 %tmp;
	  cvt.u64.u32 	%tmp, %r14;
	  cvta.shared.u64 	%rd63, %tmp; }
	mov.u64 	%rd77, %rd63;
	mov.u64 	%rd59, %rd77;
	// begin inline asm
	atom.max.relaxed.cta.s64 %rd58,[%rd59],%rd76;
	// end inline asm
	mov.u32 	%r15, 0;
	barrier.sync 	0;
	ld.shared.u64 	%rd64, [_ZZ8BlockMaxIlET_PKS0_lE9block_max];
	st.u64 	[%rd32], %rd64;
	bar.sync 	0;
	st.param.b32 	[func_retval0+0], %r15;
	ret;

}
	// .globl	BlockMax_float64
.visible .func  (.param .b32 func_retval0) BlockMax_float64(
	.param .b64 BlockMax_float64_param_0,
	.param .b64 BlockMax_float64_param_1,
	.param .b64 BlockMax_float64_param_2
)
{
	.local .align 8 .b8 	__local_depot11[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<19>;
	.reg .b32 	%r<20>;
	.reg .f64 	%fd<38>;
	.reg .b64 	%rd<69>;


	mov.u64 	%SPL, __local_depot11;
	ld.param.u64 	%rd28, [BlockMax_float64_param_0];
	ld.param.u64 	%rd29, [BlockMax_float64_param_1];
	ld.param.u64 	%rd30, [BlockMax_float64_param_2];
	mov.u32 	%r1, %ntid.y;
	mov.u32 	%r4, %tid.z;
	mov.u32 	%r5, %tid.y;
	mad.lo.s32 	%r6, %r1, %r4, %r5;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r3, %r6, %r2, %r7;
	setp.ne.s32 	%p3, %r3, 0;
	@%p3 bra 	$L__BB11_2;

	mov.u64 	%rd31, 0;
	st.shared.u64 	[_ZZ12are_all_nansIdEbRKN18cooperative_groups4__v112thread_blockEPKT_lE5count], %rd31;

$L__BB11_2:
	barrier.sync 	0;
	cvt.u64.u32 	%rd66, %r3;
	setp.ge.s64 	%p4, %rd66, %rd30;
	@%p4 bra 	$L__BB11_7;

	add.u64 	%rd2, %SPL, 0;
	mul.lo.s32 	%r8, %r2, %r1;
	mov.u32 	%r9, %ntid.z;
	mul.lo.s32 	%r10, %r8, %r9;
	cvt.u64.u32 	%rd3, %r10;
	mov.u64 	%rd61, %rd66;

$L__BB11_4:
	shl.b64 	%rd33, %rd61, 3;
	add.s64 	%rd34, %rd29, %rd33;
	ld.f64 	%fd15, [%rd34];
	abs.f64 	%fd16, %fd15;
	setp.gtu.f64 	%p5, %fd16, 0d7FF0000000000000;
	@%p5 bra 	$L__BB11_6;
	bra.uni 	$L__BB11_5;

$L__BB11_6:
	add.s64 	%rd61, %rd61, %rd3;
	setp.lt.s64 	%p6, %rd61, %rd30;
	@%p6 bra 	$L__BB11_4;
	bra.uni 	$L__BB11_7;

$L__BB11_5:
	mov.u32 	%r11, _ZZ12are_all_nansIdEbRKN18cooperative_groups4__v112thread_blockEPKT_lE5count;
	{ .reg .b64 %tmp;
	  cvt.u64.u32 	%tmp, %r11;
	  cvta.shared.u64 	%rd38, %tmp; }
	st.local.u64 	[%rd2], %rd38;
	ld.local.u64 	%rd36, [%rd2];
	mov.u64 	%rd37, 1;
	// begin inline asm
	atom.add.relaxed.cta.u64 %rd35,[%rd36],%rd37;
	// end inline asm

$L__BB11_7:
	barrier.sync 	0;
	ld.shared.u64 	%rd39, [_ZZ12are_all_nansIdEbRKN18cooperative_groups4__v112thread_blockEPKT_lE5count];
	setp.eq.s64 	%p7, %rd39, 0;
	mov.f64 	%fd37, 0d7FF8000000000000;
	@%p7 bra 	$L__BB11_25;

	@%p3 bra 	$L__BB11_10;

	mov.u64 	%rd40, -4503599627370496;
	st.shared.u64 	[_ZZ8BlockMaxIdET_PKS0_lE9block_max], %rd40;

$L__BB11_10:
	barrier.sync 	0;
	mov.f64 	%fd34, 0dFFF0000000000000;
	@%p4 bra 	$L__BB11_20;

	mul.lo.s32 	%r12, %r2, %r1;
	mov.u32 	%r13, %ntid.z;
	mul.lo.s32 	%r14, %r12, %r13;
	cvt.u64.u32 	%rd6, %r14;
	not.b64 	%rd41, %rd66;
	add.s64 	%rd7, %rd41, %rd30;
	and.b64  	%rd42, %rd7, -4294967296;
	setp.eq.s64 	%p10, %rd42, 0;
	@%p10 bra 	$L__BB11_13;

	div.u64 	%rd62, %rd7, %rd6;
	bra.uni 	$L__BB11_14;

$L__BB11_13:
	cvt.u32.u64 	%r15, %rd6;
	cvt.u32.u64 	%r16, %rd7;
	div.u32 	%r17, %r16, %r15;
	cvt.u64.u32 	%rd62, %r17;

$L__BB11_14:
	add.s64 	%rd43, %rd62, 1;
	and.b64  	%rd65, %rd43, 3;
	setp.eq.s64 	%p11, %rd65, 0;
	mov.f64 	%fd34, 0dFFF0000000000000;
	@%p11 bra 	$L__BB11_17;

	shl.b64 	%rd44, %rd66, 3;
	add.s64 	%rd63, %rd29, %rd44;
	shl.b64 	%rd13, %rd6, 3;

$L__BB11_16:
	.pragma "nounroll";
	ld.f64 	%fd22, [%rd63];
	max.f64 	%fd34, %fd34, %fd22;
	add.s64 	%rd66, %rd66, %rd6;
	add.s64 	%rd63, %rd63, %rd13;
	add.s64 	%rd65, %rd65, -1;
	setp.ne.s64 	%p12, %rd65, 0;
	@%p12 bra 	$L__BB11_16;

$L__BB11_17:
	setp.lt.u64 	%p13, %rd62, 3;
	@%p13 bra 	$L__BB11_20;

	shl.b64 	%rd45, %rd66, 3;
	add.s64 	%rd68, %rd29, %rd45;
	shl.b64 	%rd22, %rd6, 3;

$L__BB11_19:
	ld.f64 	%fd23, [%rd68];
	max.f64 	%fd24, %fd34, %fd23;
	add.s64 	%rd46, %rd68, %rd22;
	ld.f64 	%fd25, [%rd46];
	max.f64 	%fd26, %fd24, %fd25;
	add.s64 	%rd47, %rd66, %rd6;
	add.s64 	%rd48, %rd47, %rd6;
	add.s64 	%rd49, %rd46, %rd22;
	ld.f64 	%fd27, [%rd49];
	max.f64 	%fd28, %fd26, %fd27;
	add.s64 	%rd50, %rd48, %rd6;
	add.s64 	%rd51, %rd49, %rd22;
	add.s64 	%rd68, %rd51, %rd22;
	ld.f64 	%fd29, [%rd51];
	max.f64 	%fd34, %fd28, %fd29;
	add.s64 	%rd66, %rd50, %rd6;
	setp.lt.s64 	%p14, %rd66, %rd30;
	@%p14 bra 	$L__BB11_19;

$L__BB11_20:
	add.u64 	%rd55, %SPL, 0;
	mov.u32 	%r18, _ZZ8BlockMaxIdET_PKS0_lE9block_max;
	{ .reg .b64 %tmp;
	  cvt.u64.u32 	%tmp, %r18;
	  cvta.shared.u64 	%rd56, %tmp; }
	st.local.u64 	[%rd55], %rd56;
	ld.local.u64 	%rd27, [%rd55];
	// begin inline asm
	ld.relaxed.cta.b64 %rd52,[%rd27];
	// end inline asm
	mov.b64 	%fd36, %rd52;
	mov.pred 	%p15, 0;

$L__BB11_21:
	setp.gt.f64 	%p16, %fd36, %fd34;
	selp.f64 	%fd10, %fd36, %fd34, %p16;
	setp.neu.f64 	%p17, %fd10, %fd34;
	mov.pred 	%p18, %p15;
	@%p17 bra 	$L__BB11_23;

	mov.b64 	%rd59, %fd36;
	mov.b64 	%rd60, %fd10;
	// begin inline asm
	atom.cas.relaxed.cta.b64 %rd57,[%rd27],%rd59,%rd60;
	// end inline asm
	setp.ne.s64 	%p18, %rd57, %rd59;
	mov.b64 	%fd36, %rd57;

$L__BB11_23:
	@%p18 bra 	$L__BB11_21;

	barrier.sync 	0;
	ld.shared.f64 	%fd37, [_ZZ8BlockMaxIdET_PKS0_lE9block_max];

$L__BB11_25:
	st.f64 	[%rd28], %fd37;
	bar.sync 	0;
	mov.u32 	%r19, 0;
	st.param.b32 	[func_retval0+0], %r19;
	ret;

}
	// .globl	BlockIdxMin_int64
.visible .func  (.param .b32 func_retval0) BlockIdxMin_int64(
	.param .b64 BlockIdxMin_int64_param_0,
	.param .b64 BlockIdxMin_int64_param_1,
	.param .b64 BlockIdxMin_int64_param_2,
	.param .b64 BlockIdxMin_int64_param_3
)
{
	.local .align 8 .b8 	__local_depot12[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<17>;
	.reg .b16 	%rs<8>;
	.reg .b32 	%r<24>;
	.reg .b64 	%rd<139>;


	mov.u64 	%SPL, __local_depot12;
	ld.param.u64 	%rd69, [BlockIdxMin_int64_param_1];
	ld.param.u64 	%rd70, [BlockIdxMin_int64_param_2];
	ld.param.u64 	%rd71, [BlockIdxMin_int64_param_3];
	mov.u32 	%r1, %ntid.y;
	mov.u32 	%r4, %tid.z;
	mov.u32 	%r5, %tid.y;
	mad.lo.s32 	%r6, %r1, %r4, %r5;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r3, %r6, %r2, %r7;
	setp.ne.s32 	%p1, %r3, 0;
	@%p1 bra 	$L__BB12_2;

	mov.u64 	%rd72, 9223372036854775807;
	st.shared.u64 	[_ZZ11BlockIdxMinIlElPKT_PllE9block_min], %rd72;
	st.shared.u64 	[_ZZ11BlockIdxMinIlElPKT_PllE13block_idx_min], %rd72;
	mov.u16 	%rs1, 0;
	st.shared.u8 	[_ZZ11BlockIdxMinIlElPKT_PllE9found_min], %rs1;

$L__BB12_2:
	barrier.sync 	0;
	cvt.u64.u32 	%rd126, %r3;
	setp.ge.s64 	%p2, %rd126, %rd71;
	mov.u64 	%rd116, 9223372036854775807;
	mov.u64 	%rd117, %rd116;
	@%p2 bra 	$L__BB12_22;

	mul.lo.s32 	%r8, %r2, %r1;
	mov.u32 	%r9, %ntid.z;
	mul.lo.s32 	%r10, %r8, %r9;
	cvt.u64.u32 	%rd2, %r10;
	not.b64 	%rd75, %rd126;
	add.s64 	%rd3, %rd75, %rd71;
	and.b64  	%rd76, %rd3, -4294967296;
	setp.eq.s64 	%p3, %rd76, 0;
	@%p3 bra 	$L__BB12_5;

	div.u64 	%rd109, %rd3, %rd2;
	bra.uni 	$L__BB12_6;

$L__BB12_5:
	cvt.u32.u64 	%r11, %rd2;
	cvt.u32.u64 	%r12, %rd3;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd109, %r13;

$L__BB12_6:
	add.s64 	%rd7, %rd109, 1;
	and.b64  	%rd133, %rd7, 3;
	setp.lt.u64 	%p4, %rd109, 3;
	mov.u64 	%rd117, 9223372036854775807;
	mov.u64 	%rd116, %rd117;
	@%p4 bra 	$L__BB12_17;

	sub.s64 	%rd114, %rd7, %rd133;
	shl.b64 	%rd110, %rd126, 3;
	shl.b64 	%rd11, %rd2, 5;
	mul.lo.s64 	%rd82, %rd2, 24;
	add.s64 	%rd12, %rd70, %rd82;
	shl.b64 	%rd83, %rd2, 4;
	add.s64 	%rd13, %rd70, %rd83;
	shl.b64 	%rd16, %rd2, 3;
	add.s64 	%rd14, %rd70, %rd16;
	add.s64 	%rd115, %rd69, %rd110;
	mov.u16 	%rs2, 1;

$L__BB12_8:
	ld.u64 	%rd23, [%rd115];
	setp.ge.s64 	%p5, %rd23, %rd116;
	@%p5 bra 	$L__BB12_10;

	add.s64 	%rd84, %rd70, %rd110;
	ld.u64 	%rd117, [%rd84];
	st.shared.u8 	[_ZZ11BlockIdxMinIlElPKT_PllE9found_min], %rs2;
	mov.u64 	%rd116, %rd23;

$L__BB12_10:
	add.s64 	%rd27, %rd115, %rd16;
	ld.u64 	%rd28, [%rd27];
	setp.ge.s64 	%p6, %rd28, %rd116;
	@%p6 bra 	$L__BB12_12;

	add.s64 	%rd85, %rd14, %rd110;
	ld.u64 	%rd117, [%rd85];
	st.shared.u8 	[_ZZ11BlockIdxMinIlElPKT_PllE9found_min], %rs2;
	mov.u64 	%rd116, %rd28;

$L__BB12_12:
	add.s64 	%rd32, %rd27, %rd16;
	ld.u64 	%rd33, [%rd32];
	setp.ge.s64 	%p7, %rd33, %rd116;
	@%p7 bra 	$L__BB12_14;

	add.s64 	%rd86, %rd13, %rd110;
	ld.u64 	%rd117, [%rd86];
	st.shared.u8 	[_ZZ11BlockIdxMinIlElPKT_PllE9found_min], %rs2;
	mov.u64 	%rd116, %rd33;

$L__BB12_14:
	add.s64 	%rd37, %rd32, %rd16;
	ld.u64 	%rd38, [%rd37];
	setp.ge.s64 	%p8, %rd38, %rd116;
	@%p8 bra 	$L__BB12_16;

	add.s64 	%rd87, %rd12, %rd110;
	ld.u64 	%rd117, [%rd87];
	st.shared.u8 	[_ZZ11BlockIdxMinIlElPKT_PllE9found_min], %rs2;
	mov.u64 	%rd116, %rd38;

$L__BB12_16:
	add.s64 	%rd88, %rd126, %rd2;
	add.s64 	%rd89, %rd88, %rd2;
	add.s64 	%rd90, %rd89, %rd2;
	add.s64 	%rd126, %rd90, %rd2;
	add.s64 	%rd110, %rd110, %rd11;
	add.s64 	%rd114, %rd114, -4;
	setp.ne.s64 	%p9, %rd114, 0;
	add.s64 	%rd115, %rd37, %rd16;
	@%p9 bra 	$L__BB12_8;

$L__BB12_17:
	setp.eq.s64 	%p10, %rd133, 0;
	@%p10 bra 	$L__BB12_22;

	ld.param.u64 	%rd107, [BlockIdxMin_int64_param_1];
	shl.b64 	%rd91, %rd126, 3;
	add.s64 	%rd130, %rd70, %rd91;
	shl.b64 	%rd52, %rd2, 3;
	add.s64 	%rd129, %rd107, %rd91;
	mov.u16 	%rs6, 1;

$L__BB12_19:
	.pragma "nounroll";
	ld.u64 	%rd59, [%rd129];
	setp.ge.s64 	%p11, %rd59, %rd116;
	@%p11 bra 	$L__BB12_21;

	ld.u64 	%rd117, [%rd130];
	st.shared.u8 	[_ZZ11BlockIdxMinIlElPKT_PllE9found_min], %rs6;
	mov.u64 	%rd116, %rd59;

$L__BB12_21:
	add.s64 	%rd130, %rd130, %rd52;
	add.s64 	%rd129, %rd129, %rd52;
	add.s64 	%rd133, %rd133, -1;
	setp.ne.s64 	%p12, %rd133, 0;
	@%p12 bra 	$L__BB12_19;

$L__BB12_22:
	mov.u32 	%r14, _ZZ11BlockIdxMinIlElPKT_PllE9block_min;
	{ .reg .b64 %tmp;
	  cvt.u64.u32 	%tmp, %r14;
	  cvta.shared.u64 	%rd97, %tmp; }
	mov.u64 	%rd138, %rd97;
	mov.u64 	%rd93, %rd138;
	// begin inline asm
	atom.min.relaxed.cta.s64 %rd92,[%rd93],%rd116;
	// end inline asm
	barrier.sync 	0;
	ld.shared.u8 	%rs7, [_ZZ11BlockIdxMinIlElPKT_PllE9found_min];
	setp.eq.s16 	%p13, %rs7, 0;
	@%p13 bra 	$L__BB12_25;

	ld.shared.u64 	%rd98, [_ZZ11BlockIdxMinIlElPKT_PllE9block_min];
	setp.ne.s64 	%p14, %rd116, %rd98;
	@%p14 bra 	$L__BB12_27;

	add.u64 	%rd103, %SPL, 0;
	mov.u32 	%r15, _ZZ11BlockIdxMinIlElPKT_PllE13block_idx_min;
	{ .reg .b64 %tmp;
	  cvt.u64.u32 	%tmp, %r15;
	  cvta.shared.u64 	%rd104, %tmp; }
	st.local.u64 	[%rd103], %rd104;
	ld.local.u64 	%rd100, [%rd103];
	// begin inline asm
	atom.min.relaxed.cta.s64 %rd99,[%rd100],%rd117;
	// end inline asm
	bra.uni 	$L__BB12_27;

$L__BB12_25:
	mov.u32 	%r23, %tid.y;
	mov.u32 	%r22, %tid.z;
	mov.u32 	%r21, %ntid.y;
	mov.u32 	%r20, %tid.x;
	mov.u32 	%r19, %ntid.x;
	mad.lo.s32 	%r18, %r21, %r22, %r23;
	mad.lo.s32 	%r17, %r18, %r19, %r20;
	setp.ne.s32 	%p16, %r17, 0;
	@%p16 bra 	$L__BB12_27;

	ld.u64 	%rd105, [%rd70];
	st.shared.u64 	[_ZZ11BlockIdxMinIlElPKT_PllE13block_idx_min], %rd105;

$L__BB12_27:
	ld.param.u64 	%rd108, [BlockIdxMin_int64_param_0];
	mov.u32 	%r16, 0;
	barrier.sync 	0;
	ld.shared.u64 	%rd106, [_ZZ11BlockIdxMinIlElPKT_PllE13block_idx_min];
	st.u64 	[%rd108], %rd106;
	bar.sync 	0;
	st.param.b32 	[func_retval0+0], %r16;
	ret;

}
	// .globl	BlockIdxMin_float64
.visible .func  (.param .b32 func_retval0) BlockIdxMin_float64(
	.param .b64 BlockIdxMin_float64_param_0,
	.param .b64 BlockIdxMin_float64_param_1,
	.param .b64 BlockIdxMin_float64_param_2,
	.param .b64 BlockIdxMin_float64_param_3
)
{
	.reg .pred 	%p<23>;
	.reg .b16 	%rs<8>;
	.reg .b32 	%r<24>;
	.reg .f64 	%fd<38>;
	.reg .b64 	%rd<110>;


	ld.param.u64 	%rd51, [BlockIdxMin_float64_param_0];
	ld.param.u64 	%rd52, [BlockIdxMin_float64_param_1];
	ld.param.u64 	%rd53, [BlockIdxMin_float64_param_2];
	ld.param.u64 	%rd54, [BlockIdxMin_float64_param_3];
	mov.u32 	%r1, %ntid.y;
	mov.u32 	%r4, %tid.z;
	mov.u32 	%r5, %tid.y;
	mad.lo.s32 	%r6, %r1, %r4, %r5;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r3, %r6, %r2, %r7;
	setp.ne.s32 	%p3, %r3, 0;
	@%p3 bra 	$L__BB13_2;

	mov.u64 	%rd55, 9218868437227405312;
	st.shared.u64 	[_ZZ11BlockIdxMinIdElPKT_PllE9block_min], %rd55;
	mov.u64 	%rd56, 9223372036854775807;
	st.shared.u64 	[_ZZ11BlockIdxMinIdElPKT_PllE13block_idx_min], %rd56;
	mov.u16 	%rs1, 0;
	st.shared.u8 	[_ZZ11BlockIdxMinIdElPKT_PllE9found_min], %rs1;

$L__BB13_2:
	barrier.sync 	0;
	cvt.u64.u32 	%rd100, %r3;
	setp.ge.s64 	%p4, %rd100, %rd54;
	mov.u64 	%rd95, 9223372036854775807;
	mov.f64 	%fd27, 0d7FF0000000000000;
	@%p4 bra 	$L__BB13_22;

	mul.lo.s32 	%r8, %r2, %r1;
	mov.u32 	%r9, %ntid.z;
	mul.lo.s32 	%r10, %r8, %r9;
	cvt.u64.u32 	%rd2, %r10;
	not.b64 	%rd58, %rd100;
	add.s64 	%rd3, %rd58, %rd54;
	and.b64  	%rd59, %rd3, -4294967296;
	setp.eq.s64 	%p5, %rd59, 0;
	@%p5 bra 	$L__BB13_5;

	div.u64 	%rd90, %rd3, %rd2;
	bra.uni 	$L__BB13_6;

$L__BB13_5:
	cvt.u32.u64 	%r11, %rd2;
	cvt.u32.u64 	%r12, %rd3;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd90, %r13;

$L__BB13_6:
	add.s64 	%rd7, %rd90, 1;
	and.b64  	%rd105, %rd7, 3;
	setp.lt.u64 	%p6, %rd90, 3;
	mov.f64 	%fd27, 0d7FF0000000000000;
	mov.u64 	%rd95, 9223372036854775807;
	@%p6 bra 	$L__BB13_17;

	sub.s64 	%rd93, %rd7, %rd105;
	shl.b64 	%rd63, %rd100, 3;
	add.s64 	%rd94, %rd52, %rd63;
	shl.b64 	%rd11, %rd2, 3;
	mov.u16 	%rs2, 1;

$L__BB13_8:
	ld.f64 	%fd2, [%rd94];
	setp.geu.f64 	%p7, %fd2, %fd27;
	@%p7 bra 	$L__BB13_10;

	shl.b64 	%rd64, %rd100, 3;
	add.s64 	%rd65, %rd53, %rd64;
	ld.u64 	%rd95, [%rd65];
	st.shared.u8 	[_ZZ11BlockIdxMinIdElPKT_PllE9found_min], %rs2;
	mov.f64 	%fd27, %fd2;

$L__BB13_10:
	add.s64 	%rd18, %rd100, %rd2;
	add.s64 	%rd19, %rd94, %rd11;
	ld.f64 	%fd4, [%rd19];
	setp.geu.f64 	%p8, %fd4, %fd27;
	@%p8 bra 	$L__BB13_12;

	shl.b64 	%rd66, %rd18, 3;
	add.s64 	%rd67, %rd53, %rd66;
	ld.u64 	%rd95, [%rd67];
	st.shared.u8 	[_ZZ11BlockIdxMinIdElPKT_PllE9found_min], %rs2;
	mov.f64 	%fd27, %fd4;

$L__BB13_12:
	add.s64 	%rd22, %rd18, %rd2;
	add.s64 	%rd23, %rd19, %rd11;
	ld.f64 	%fd6, [%rd23];
	setp.geu.f64 	%p9, %fd6, %fd27;
	@%p9 bra 	$L__BB13_14;

	shl.b64 	%rd68, %rd22, 3;
	add.s64 	%rd69, %rd53, %rd68;
	ld.u64 	%rd95, [%rd69];
	st.shared.u8 	[_ZZ11BlockIdxMinIdElPKT_PllE9found_min], %rs2;
	mov.f64 	%fd27, %fd6;

$L__BB13_14:
	add.s64 	%rd26, %rd22, %rd2;
	add.s64 	%rd27, %rd23, %rd11;
	ld.f64 	%fd8, [%rd27];
	setp.geu.f64 	%p10, %fd8, %fd27;
	@%p10 bra 	$L__BB13_16;

	shl.b64 	%rd70, %rd26, 3;
	add.s64 	%rd71, %rd53, %rd70;
	ld.u64 	%rd95, [%rd71];
	st.shared.u8 	[_ZZ11BlockIdxMinIdElPKT_PllE9found_min], %rs2;
	mov.f64 	%fd27, %fd8;

$L__BB13_16:
	add.s64 	%rd100, %rd26, %rd2;
	add.s64 	%rd93, %rd93, -4;
	setp.ne.s64 	%p11, %rd93, 0;
	add.s64 	%rd94, %rd27, %rd11;
	@%p11 bra 	$L__BB13_8;

$L__BB13_17:
	setp.eq.s64 	%p12, %rd105, 0;
	@%p12 bra 	$L__BB13_22;

	ld.param.u64 	%rd89, [BlockIdxMin_float64_param_1];
	shl.b64 	%rd72, %rd100, 3;
	add.s64 	%rd103, %rd53, %rd72;
	shl.b64 	%rd37, %rd2, 3;
	add.s64 	%rd102, %rd89, %rd72;
	mov.u16 	%rs6, 1;

$L__BB13_19:
	.pragma "nounroll";
	ld.f64 	%fd13, [%rd102];
	setp.geu.f64 	%p13, %fd13, %fd27;
	@%p13 bra 	$L__BB13_21;

	ld.u64 	%rd95, [%rd103];
	st.shared.u8 	[_ZZ11BlockIdxMinIdElPKT_PllE9found_min], %rs6;
	mov.f64 	%fd27, %fd13;

$L__BB13_21:
	add.s64 	%rd103, %rd103, %rd37;
	add.s64 	%rd102, %rd102, %rd37;
	add.s64 	%rd105, %rd105, -1;
	setp.ne.s64 	%p14, %rd105, 0;
	@%p14 bra 	$L__BB13_19;

$L__BB13_22:
	mov.u32 	%r14, _ZZ11BlockIdxMinIdElPKT_PllE9block_min;
	{ .reg .b64 %tmp;
	  cvt.u64.u32 	%tmp, %r14;
	  cvta.shared.u64 	%rd78, %tmp; }
	mov.u64 	%rd108, %rd78;
	mov.u64 	%rd50, %rd108;
	// begin inline asm
	ld.relaxed.cta.b64 %rd73,[%rd50];
	// end inline asm
	mov.b64 	%fd37, %rd73;
	mov.pred 	%p15, 0;

$L__BB13_23:
	setp.lt.f64 	%p16, %fd37, %fd27;
	selp.f64 	%fd18, %fd37, %fd27, %p16;
	setp.neu.f64 	%p17, %fd18, %fd27;
	mov.pred 	%p22, %p15;
	@%p17 bra 	$L__BB13_25;

	mov.b64 	%rd81, %fd37;
	mov.b64 	%rd82, %fd18;
	// begin inline asm
	atom.cas.relaxed.cta.b64 %rd79,[%rd50],%rd81,%rd82;
	// end inline asm
	setp.ne.s64 	%p22, %rd79, %rd81;
	mov.b64 	%fd37, %rd79;

$L__BB13_25:
	@%p22 bra 	$L__BB13_23;

	barrier.sync 	0;
	ld.shared.u8 	%rs7, [_ZZ11BlockIdxMinIdElPKT_PllE9found_min];
	setp.eq.s16 	%p18, %rs7, 0;
	@%p18 bra 	$L__BB13_29;

	ld.shared.f64 	%fd25, [_ZZ11BlockIdxMinIdElPKT_PllE9block_min];
	setp.neu.f64 	%p19, %fd27, %fd25;
	@%p19 bra 	$L__BB13_31;

	mov.u32 	%r15, _ZZ11BlockIdxMinIdElPKT_PllE13block_idx_min;
	{ .reg .b64 %tmp;
	  cvt.u64.u32 	%tmp, %r15;
	  cvta.shared.u64 	%rd86, %tmp; }
	mov.u64 	%rd109, %rd86;
	mov.u64 	%rd84, %rd109;
	// begin inline asm
	atom.min.relaxed.cta.s64 %rd83,[%rd84],%rd95;
	// end inline asm
	bra.uni 	$L__BB13_31;

$L__BB13_29:
	mov.u32 	%r23, %tid.y;
	mov.u32 	%r22, %tid.z;
	mov.u32 	%r21, %ntid.y;
	mov.u32 	%r20, %tid.x;
	mov.u32 	%r19, %ntid.x;
	mad.lo.s32 	%r18, %r21, %r22, %r23;
	mad.lo.s32 	%r17, %r18, %r19, %r20;
	setp.ne.s32 	%p21, %r17, 0;
	@%p21 bra 	$L__BB13_31;

	ld.u64 	%rd87, [%rd53];
	st.shared.u64 	[_ZZ11BlockIdxMinIdElPKT_PllE13block_idx_min], %rd87;

$L__BB13_31:
	mov.u32 	%r16, 0;
	barrier.sync 	0;
	ld.shared.u64 	%rd88, [_ZZ11BlockIdxMinIdElPKT_PllE13block_idx_min];
	st.u64 	[%rd51], %rd88;
	bar.sync 	0;
	st.param.b32 	[func_retval0+0], %r16;
	ret;

}
	// .globl	BlockIdxMax_int64
.visible .func  (.param .b32 func_retval0) BlockIdxMax_int64(
	.param .b64 BlockIdxMax_int64_param_0,
	.param .b64 BlockIdxMax_int64_param_1,
	.param .b64 BlockIdxMax_int64_param_2,
	.param .b64 BlockIdxMax_int64_param_3
)
{
	.local .align 8 .b8 	__local_depot14[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<17>;
	.reg .b16 	%rs<8>;
	.reg .b32 	%r<24>;
	.reg .b64 	%rd<140>;


	mov.u64 	%SPL, __local_depot14;
	ld.param.u64 	%rd69, [BlockIdxMax_int64_param_1];
	ld.param.u64 	%rd70, [BlockIdxMax_int64_param_2];
	ld.param.u64 	%rd71, [BlockIdxMax_int64_param_3];
	mov.u32 	%r1, %ntid.y;
	mov.u32 	%r4, %tid.z;
	mov.u32 	%r5, %tid.y;
	mad.lo.s32 	%r6, %r1, %r4, %r5;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r3, %r6, %r2, %r7;
	setp.ne.s32 	%p1, %r3, 0;
	@%p1 bra 	$L__BB14_2;

	mov.u64 	%rd72, -9223372036854775808;
	st.shared.u64 	[_ZZ11BlockIdxMaxIlElPKT_PllE9block_max], %rd72;
	mov.u64 	%rd73, 9223372036854775807;
	st.shared.u64 	[_ZZ11BlockIdxMaxIlElPKT_PllE13block_idx_max], %rd73;
	mov.u16 	%rs1, 0;
	st.shared.u8 	[_ZZ11BlockIdxMaxIlElPKT_PllE9found_max], %rs1;

$L__BB14_2:
	barrier.sync 	0;
	cvt.u64.u32 	%rd127, %r3;
	setp.ge.s64 	%p2, %rd127, %rd71;
	mov.u64 	%rd118, 9223372036854775807;
	mov.u64 	%rd117, -9223372036854775808;
	@%p2 bra 	$L__BB14_22;

	mul.lo.s32 	%r8, %r2, %r1;
	mov.u32 	%r9, %ntid.z;
	mul.lo.s32 	%r10, %r8, %r9;
	cvt.u64.u32 	%rd2, %r10;
	not.b64 	%rd76, %rd127;
	add.s64 	%rd3, %rd76, %rd71;
	and.b64  	%rd77, %rd3, -4294967296;
	setp.eq.s64 	%p3, %rd77, 0;
	@%p3 bra 	$L__BB14_5;

	div.u64 	%rd110, %rd3, %rd2;
	bra.uni 	$L__BB14_6;

$L__BB14_5:
	cvt.u32.u64 	%r11, %rd2;
	cvt.u32.u64 	%r12, %rd3;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd110, %r13;

$L__BB14_6:
	add.s64 	%rd7, %rd110, 1;
	and.b64  	%rd134, %rd7, 3;
	setp.lt.u64 	%p4, %rd110, 3;
	mov.u64 	%rd117, -9223372036854775808;
	mov.u64 	%rd118, 9223372036854775807;
	@%p4 bra 	$L__BB14_17;

	sub.s64 	%rd115, %rd7, %rd134;
	shl.b64 	%rd111, %rd127, 3;
	shl.b64 	%rd11, %rd2, 5;
	mul.lo.s64 	%rd83, %rd2, 24;
	add.s64 	%rd12, %rd70, %rd83;
	shl.b64 	%rd84, %rd2, 4;
	add.s64 	%rd13, %rd70, %rd84;
	shl.b64 	%rd16, %rd2, 3;
	add.s64 	%rd14, %rd70, %rd16;
	add.s64 	%rd116, %rd69, %rd111;
	mov.u16 	%rs2, 1;

$L__BB14_8:
	ld.u64 	%rd23, [%rd116];
	setp.le.s64 	%p5, %rd23, %rd117;
	@%p5 bra 	$L__BB14_10;

	add.s64 	%rd85, %rd70, %rd111;
	ld.u64 	%rd118, [%rd85];
	st.shared.u8 	[_ZZ11BlockIdxMaxIlElPKT_PllE9found_max], %rs2;
	mov.u64 	%rd117, %rd23;

$L__BB14_10:
	add.s64 	%rd27, %rd116, %rd16;
	ld.u64 	%rd28, [%rd27];
	setp.le.s64 	%p6, %rd28, %rd117;
	@%p6 bra 	$L__BB14_12;

	add.s64 	%rd86, %rd14, %rd111;
	ld.u64 	%rd118, [%rd86];
	st.shared.u8 	[_ZZ11BlockIdxMaxIlElPKT_PllE9found_max], %rs2;
	mov.u64 	%rd117, %rd28;

$L__BB14_12:
	add.s64 	%rd32, %rd27, %rd16;
	ld.u64 	%rd33, [%rd32];
	setp.le.s64 	%p7, %rd33, %rd117;
	@%p7 bra 	$L__BB14_14;

	add.s64 	%rd87, %rd13, %rd111;
	ld.u64 	%rd118, [%rd87];
	st.shared.u8 	[_ZZ11BlockIdxMaxIlElPKT_PllE9found_max], %rs2;
	mov.u64 	%rd117, %rd33;

$L__BB14_14:
	add.s64 	%rd37, %rd32, %rd16;
	ld.u64 	%rd38, [%rd37];
	setp.le.s64 	%p8, %rd38, %rd117;
	@%p8 bra 	$L__BB14_16;

	add.s64 	%rd88, %rd12, %rd111;
	ld.u64 	%rd118, [%rd88];
	st.shared.u8 	[_ZZ11BlockIdxMaxIlElPKT_PllE9found_max], %rs2;
	mov.u64 	%rd117, %rd38;

$L__BB14_16:
	add.s64 	%rd89, %rd127, %rd2;
	add.s64 	%rd90, %rd89, %rd2;
	add.s64 	%rd91, %rd90, %rd2;
	add.s64 	%rd127, %rd91, %rd2;
	add.s64 	%rd111, %rd111, %rd11;
	add.s64 	%rd115, %rd115, -4;
	setp.ne.s64 	%p9, %rd115, 0;
	add.s64 	%rd116, %rd37, %rd16;
	@%p9 bra 	$L__BB14_8;

$L__BB14_17:
	setp.eq.s64 	%p10, %rd134, 0;
	@%p10 bra 	$L__BB14_22;

	ld.param.u64 	%rd108, [BlockIdxMax_int64_param_1];
	shl.b64 	%rd92, %rd127, 3;
	add.s64 	%rd131, %rd70, %rd92;
	shl.b64 	%rd52, %rd2, 3;
	add.s64 	%rd130, %rd108, %rd92;
	mov.u16 	%rs6, 1;

$L__BB14_19:
	.pragma "nounroll";
	ld.u64 	%rd59, [%rd130];
	setp.le.s64 	%p11, %rd59, %rd117;
	@%p11 bra 	$L__BB14_21;

	ld.u64 	%rd118, [%rd131];
	st.shared.u8 	[_ZZ11BlockIdxMaxIlElPKT_PllE9found_max], %rs6;
	mov.u64 	%rd117, %rd59;

$L__BB14_21:
	add.s64 	%rd131, %rd131, %rd52;
	add.s64 	%rd130, %rd130, %rd52;
	add.s64 	%rd134, %rd134, -1;
	setp.ne.s64 	%p12, %rd134, 0;
	@%p12 bra 	$L__BB14_19;

$L__BB14_22:
	mov.u32 	%r14, _ZZ11BlockIdxMaxIlElPKT_PllE9block_max;
	{ .reg .b64 %tmp;
	  cvt.u64.u32 	%tmp, %r14;
	  cvta.shared.u64 	%rd98, %tmp; }
	mov.u64 	%rd139, %rd98;
	mov.u64 	%rd94, %rd139;
	// begin inline asm
	atom.max.relaxed.cta.s64 %rd93,[%rd94],%rd117;
	// end inline asm
	barrier.sync 	0;
	ld.shared.u8 	%rs7, [_ZZ11BlockIdxMaxIlElPKT_PllE9found_max];
	setp.eq.s16 	%p13, %rs7, 0;
	@%p13 bra 	$L__BB14_25;

	ld.shared.u64 	%rd99, [_ZZ11BlockIdxMaxIlElPKT_PllE9block_max];
	setp.ne.s64 	%p14, %rd117, %rd99;
	@%p14 bra 	$L__BB14_27;

	add.u64 	%rd104, %SPL, 0;
	mov.u32 	%r15, _ZZ11BlockIdxMaxIlElPKT_PllE13block_idx_max;
	{ .reg .b64 %tmp;
	  cvt.u64.u32 	%tmp, %r15;
	  cvta.shared.u64 	%rd105, %tmp; }
	st.local.u64 	[%rd104], %rd105;
	ld.local.u64 	%rd101, [%rd104];
	// begin inline asm
	atom.min.relaxed.cta.s64 %rd100,[%rd101],%rd118;
	// end inline asm
	bra.uni 	$L__BB14_27;

$L__BB14_25:
	mov.u32 	%r23, %tid.y;
	mov.u32 	%r22, %tid.z;
	mov.u32 	%r21, %ntid.y;
	mov.u32 	%r20, %tid.x;
	mov.u32 	%r19, %ntid.x;
	mad.lo.s32 	%r18, %r21, %r22, %r23;
	mad.lo.s32 	%r17, %r18, %r19, %r20;
	setp.ne.s32 	%p16, %r17, 0;
	@%p16 bra 	$L__BB14_27;

	ld.u64 	%rd106, [%rd70];
	st.shared.u64 	[_ZZ11BlockIdxMaxIlElPKT_PllE13block_idx_max], %rd106;

$L__BB14_27:
	ld.param.u64 	%rd109, [BlockIdxMax_int64_param_0];
	mov.u32 	%r16, 0;
	barrier.sync 	0;
	ld.shared.u64 	%rd107, [_ZZ11BlockIdxMaxIlElPKT_PllE13block_idx_max];
	st.u64 	[%rd109], %rd107;
	bar.sync 	0;
	st.param.b32 	[func_retval0+0], %r16;
	ret;

}
	// .globl	BlockIdxMax_float64
.visible .func  (.param .b32 func_retval0) BlockIdxMax_float64(
	.param .b64 BlockIdxMax_float64_param_0,
	.param .b64 BlockIdxMax_float64_param_1,
	.param .b64 BlockIdxMax_float64_param_2,
	.param .b64 BlockIdxMax_float64_param_3
)
{
	.reg .pred 	%p<23>;
	.reg .b16 	%rs<8>;
	.reg .b32 	%r<24>;
	.reg .f64 	%fd<38>;
	.reg .b64 	%rd<110>;


	ld.param.u64 	%rd51, [BlockIdxMax_float64_param_0];
	ld.param.u64 	%rd52, [BlockIdxMax_float64_param_1];
	ld.param.u64 	%rd53, [BlockIdxMax_float64_param_2];
	ld.param.u64 	%rd54, [BlockIdxMax_float64_param_3];
	mov.u32 	%r1, %ntid.y;
	mov.u32 	%r4, %tid.z;
	mov.u32 	%r5, %tid.y;
	mad.lo.s32 	%r6, %r1, %r4, %r5;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r3, %r6, %r2, %r7;
	setp.ne.s32 	%p3, %r3, 0;
	@%p3 bra 	$L__BB15_2;

	mov.u64 	%rd55, -4503599627370496;
	st.shared.u64 	[_ZZ11BlockIdxMaxIdElPKT_PllE9block_max], %rd55;
	mov.u64 	%rd56, 9223372036854775807;
	st.shared.u64 	[_ZZ11BlockIdxMaxIdElPKT_PllE13block_idx_max], %rd56;
	mov.u16 	%rs1, 0;
	st.shared.u8 	[_ZZ11BlockIdxMaxIdElPKT_PllE9found_max], %rs1;

$L__BB15_2:
	barrier.sync 	0;
	cvt.u64.u32 	%rd100, %r3;
	setp.ge.s64 	%p4, %rd100, %rd54;
	mov.u64 	%rd95, 9223372036854775807;
	mov.f64 	%fd27, 0dFFF0000000000000;
	@%p4 bra 	$L__BB15_22;

	mul.lo.s32 	%r8, %r2, %r1;
	mov.u32 	%r9, %ntid.z;
	mul.lo.s32 	%r10, %r8, %r9;
	cvt.u64.u32 	%rd2, %r10;
	not.b64 	%rd58, %rd100;
	add.s64 	%rd3, %rd58, %rd54;
	and.b64  	%rd59, %rd3, -4294967296;
	setp.eq.s64 	%p5, %rd59, 0;
	@%p5 bra 	$L__BB15_5;

	div.u64 	%rd90, %rd3, %rd2;
	bra.uni 	$L__BB15_6;

$L__BB15_5:
	cvt.u32.u64 	%r11, %rd2;
	cvt.u32.u64 	%r12, %rd3;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd90, %r13;

$L__BB15_6:
	add.s64 	%rd7, %rd90, 1;
	and.b64  	%rd105, %rd7, 3;
	setp.lt.u64 	%p6, %rd90, 3;
	mov.f64 	%fd27, 0dFFF0000000000000;
	mov.u64 	%rd95, 9223372036854775807;
	@%p6 bra 	$L__BB15_17;

	sub.s64 	%rd93, %rd7, %rd105;
	shl.b64 	%rd63, %rd100, 3;
	add.s64 	%rd94, %rd52, %rd63;
	shl.b64 	%rd11, %rd2, 3;
	mov.u16 	%rs2, 1;

$L__BB15_8:
	ld.f64 	%fd2, [%rd94];
	setp.leu.f64 	%p7, %fd2, %fd27;
	@%p7 bra 	$L__BB15_10;

	shl.b64 	%rd64, %rd100, 3;
	add.s64 	%rd65, %rd53, %rd64;
	ld.u64 	%rd95, [%rd65];
	st.shared.u8 	[_ZZ11BlockIdxMaxIdElPKT_PllE9found_max], %rs2;
	mov.f64 	%fd27, %fd2;

$L__BB15_10:
	add.s64 	%rd18, %rd100, %rd2;
	add.s64 	%rd19, %rd94, %rd11;
	ld.f64 	%fd4, [%rd19];
	setp.leu.f64 	%p8, %fd4, %fd27;
	@%p8 bra 	$L__BB15_12;

	shl.b64 	%rd66, %rd18, 3;
	add.s64 	%rd67, %rd53, %rd66;
	ld.u64 	%rd95, [%rd67];
	st.shared.u8 	[_ZZ11BlockIdxMaxIdElPKT_PllE9found_max], %rs2;
	mov.f64 	%fd27, %fd4;

$L__BB15_12:
	add.s64 	%rd22, %rd18, %rd2;
	add.s64 	%rd23, %rd19, %rd11;
	ld.f64 	%fd6, [%rd23];
	setp.leu.f64 	%p9, %fd6, %fd27;
	@%p9 bra 	$L__BB15_14;

	shl.b64 	%rd68, %rd22, 3;
	add.s64 	%rd69, %rd53, %rd68;
	ld.u64 	%rd95, [%rd69];
	st.shared.u8 	[_ZZ11BlockIdxMaxIdElPKT_PllE9found_max], %rs2;
	mov.f64 	%fd27, %fd6;

$L__BB15_14:
	add.s64 	%rd26, %rd22, %rd2;
	add.s64 	%rd27, %rd23, %rd11;
	ld.f64 	%fd8, [%rd27];
	setp.leu.f64 	%p10, %fd8, %fd27;
	@%p10 bra 	$L__BB15_16;

	shl.b64 	%rd70, %rd26, 3;
	add.s64 	%rd71, %rd53, %rd70;
	ld.u64 	%rd95, [%rd71];
	st.shared.u8 	[_ZZ11BlockIdxMaxIdElPKT_PllE9found_max], %rs2;
	mov.f64 	%fd27, %fd8;

$L__BB15_16:
	add.s64 	%rd100, %rd26, %rd2;
	add.s64 	%rd93, %rd93, -4;
	setp.ne.s64 	%p11, %rd93, 0;
	add.s64 	%rd94, %rd27, %rd11;
	@%p11 bra 	$L__BB15_8;

$L__BB15_17:
	setp.eq.s64 	%p12, %rd105, 0;
	@%p12 bra 	$L__BB15_22;

	ld.param.u64 	%rd89, [BlockIdxMax_float64_param_1];
	shl.b64 	%rd72, %rd100, 3;
	add.s64 	%rd103, %rd53, %rd72;
	shl.b64 	%rd37, %rd2, 3;
	add.s64 	%rd102, %rd89, %rd72;
	mov.u16 	%rs6, 1;

$L__BB15_19:
	.pragma "nounroll";
	ld.f64 	%fd13, [%rd102];
	setp.leu.f64 	%p13, %fd13, %fd27;
	@%p13 bra 	$L__BB15_21;

	ld.u64 	%rd95, [%rd103];
	st.shared.u8 	[_ZZ11BlockIdxMaxIdElPKT_PllE9found_max], %rs6;
	mov.f64 	%fd27, %fd13;

$L__BB15_21:
	add.s64 	%rd103, %rd103, %rd37;
	add.s64 	%rd102, %rd102, %rd37;
	add.s64 	%rd105, %rd105, -1;
	setp.ne.s64 	%p14, %rd105, 0;
	@%p14 bra 	$L__BB15_19;

$L__BB15_22:
	mov.u32 	%r14, _ZZ11BlockIdxMaxIdElPKT_PllE9block_max;
	{ .reg .b64 %tmp;
	  cvt.u64.u32 	%tmp, %r14;
	  cvta.shared.u64 	%rd78, %tmp; }
	mov.u64 	%rd108, %rd78;
	mov.u64 	%rd50, %rd108;
	// begin inline asm
	ld.relaxed.cta.b64 %rd73,[%rd50];
	// end inline asm
	mov.b64 	%fd37, %rd73;
	mov.pred 	%p15, 0;

$L__BB15_23:
	setp.gt.f64 	%p16, %fd37, %fd27;
	selp.f64 	%fd18, %fd37, %fd27, %p16;
	setp.neu.f64 	%p17, %fd18, %fd27;
	mov.pred 	%p22, %p15;
	@%p17 bra 	$L__BB15_25;

	mov.b64 	%rd81, %fd37;
	mov.b64 	%rd82, %fd18;
	// begin inline asm
	atom.cas.relaxed.cta.b64 %rd79,[%rd50],%rd81,%rd82;
	// end inline asm
	setp.ne.s64 	%p22, %rd79, %rd81;
	mov.b64 	%fd37, %rd79;

$L__BB15_25:
	@%p22 bra 	$L__BB15_23;

	barrier.sync 	0;
	ld.shared.u8 	%rs7, [_ZZ11BlockIdxMaxIdElPKT_PllE9found_max];
	setp.eq.s16 	%p18, %rs7, 0;
	@%p18 bra 	$L__BB15_29;

	ld.shared.f64 	%fd25, [_ZZ11BlockIdxMaxIdElPKT_PllE9block_max];
	setp.neu.f64 	%p19, %fd27, %fd25;
	@%p19 bra 	$L__BB15_31;

	mov.u32 	%r15, _ZZ11BlockIdxMaxIdElPKT_PllE13block_idx_max;
	{ .reg .b64 %tmp;
	  cvt.u64.u32 	%tmp, %r15;
	  cvta.shared.u64 	%rd86, %tmp; }
	mov.u64 	%rd109, %rd86;
	mov.u64 	%rd84, %rd109;
	// begin inline asm
	atom.min.relaxed.cta.s64 %rd83,[%rd84],%rd95;
	// end inline asm
	bra.uni 	$L__BB15_31;

$L__BB15_29:
	mov.u32 	%r23, %tid.y;
	mov.u32 	%r22, %tid.z;
	mov.u32 	%r21, %ntid.y;
	mov.u32 	%r20, %tid.x;
	mov.u32 	%r19, %ntid.x;
	mad.lo.s32 	%r18, %r21, %r22, %r23;
	mad.lo.s32 	%r17, %r18, %r19, %r20;
	setp.ne.s32 	%p21, %r17, 0;
	@%p21 bra 	$L__BB15_31;

	ld.u64 	%rd87, [%rd53];
	st.shared.u64 	[_ZZ11BlockIdxMaxIdElPKT_PllE13block_idx_max], %rd87;

$L__BB15_31:
	mov.u32 	%r16, 0;
	barrier.sync 	0;
	ld.shared.u64 	%rd88, [_ZZ11BlockIdxMaxIdElPKT_PllE13block_idx_max];
	st.u64 	[%rd51], %rd88;
	bar.sync 	0;
	st.param.b32 	[func_retval0+0], %r16;
	ret;

}
	// .weak	_ZN3cub17CUB_101702_750_NS11EmptyKernelIvEEvv
.weak .entry _ZN3cub17CUB_101702_750_NS11EmptyKernelIvEEvv()
{



	ret;

}

